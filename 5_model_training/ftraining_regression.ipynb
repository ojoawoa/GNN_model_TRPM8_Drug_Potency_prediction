{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch torch-geometric scikit-learn pandas numpy matplotlib rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWen_C8dKLiL",
        "outputId": "01f5cb89-cf01-49b8-9beb-326ee9503647"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit-pypi, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rdkit-pypi-2022.9.5 torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Drive & Imports\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark    = False\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    wseed = seed + worker_id\n",
        "    np.random.seed(wseed)\n",
        "    random.seed(wseed)\n",
        "\n",
        "generator = torch.Generator().manual_seed(seed)\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 📁 Paths\n",
        "root_dir    = \"/content/drive/MyDrive/GNN_model_TRPM8_Drug_Potency_prediction/GNN_model_TRPM8_Drug_Potency_prediction-Dolapo\"\n",
        "base_path   = os.path.join(root_dir, \"4_train_test_split/10fold_cv/regression\")\n",
        "results_dir = os.path.join(root_dir, \"GCN_results/regression_10fold\")\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "kfold = 10\n",
        "task  = \"regression\"\n",
        "\n",
        "# Step 2: GCN Regression Model\n",
        "class GCNReg(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.conv1   = GCNConv(in_dim, hidden_dim)\n",
        "        self.conv2   = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.dropout = dropout\n",
        "        self.lin     = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# Step 3: Evaluation helper for regression\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out   = model(batch)          # shape [batch_size,1]\n",
        "            preds.append(out.cpu().view(-1))\n",
        "            labels.append(batch.y.cpu().view(-1))\n",
        "    preds  = torch.cat(preds)\n",
        "    labels = torch.cat(labels)\n",
        "    return preds, labels\n",
        "\n",
        "# Step 4: Hyperparameter Sweep (10-Fold CV)\n",
        "hidden_dims = [64, 128]\n",
        "dropouts    = [0.0, 0.2]\n",
        "lrs         = [1e-3, 5e-4]\n",
        "\n",
        "sweep_results = []\n",
        "for hd in hidden_dims:\n",
        "    for dp in dropouts:\n",
        "        for lr in lrs:\n",
        "            fold_mses = []\n",
        "            for fold in range(kfold):\n",
        "                # load splits with weights_only disabled\n",
        "                tr_data = torch.load(\n",
        "                    os.path.join(base_path, f\"{task}_train_fold{fold}.pt\"),\n",
        "                    weights_only=False\n",
        "                )\n",
        "                vl_data = torch.load(\n",
        "                    os.path.join(base_path, f\"{task}_val_fold{fold}.pt\"),\n",
        "                    weights_only=False\n",
        "                )\n",
        "\n",
        "                tr_loader = DataLoader(\n",
        "                    tr_data, batch_size=32, shuffle=True,\n",
        "                    worker_init_fn=seed_worker, generator=generator\n",
        "                )\n",
        "                vl_loader = DataLoader(vl_data, batch_size=32)\n",
        "\n",
        "                # init model & optimizer\n",
        "                model     = GCNReg(\n",
        "                    in_dim=tr_data[0].x.size(1),\n",
        "                    hidden_dim=hd,\n",
        "                    dropout=dp\n",
        "                ).to(device)\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                # train for 50 epochs\n",
        "                for epoch in range(50):\n",
        "                    model.train()\n",
        "                    for batch in tr_loader:\n",
        "                        batch = batch.to(device)\n",
        "                        optimizer.zero_grad()\n",
        "                        out    = model(batch).view(-1)\n",
        "                        target = batch.y.view(-1).float()\n",
        "                        loss   = F.mse_loss(out, target)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # validation MSE\n",
        "                preds, labels = evaluate(model, vl_loader)\n",
        "                mse = mean_squared_error(labels.numpy(), preds.numpy())\n",
        "                fold_mses.append(mse)\n",
        "\n",
        "            mean_mse = np.mean(fold_mses)\n",
        "            std_mse  = np.std(fold_mses)\n",
        "            sweep_results.append({\n",
        "                \"hidden_dim\": hd,\n",
        "                \"dropout\":    dp,\n",
        "                \"lr\":         lr,\n",
        "                \"mean_mse\":   mean_mse,\n",
        "                \"std_mse\":    std_mse\n",
        "            })\n",
        "            print(f\"hd={hd} dp={dp} lr={lr} → MSE {mean_mse:.4f} ± {std_mse:.4f}\")\n",
        "\n",
        "sweep_df = pd.DataFrame(sweep_results)\n",
        "sweep_df.to_csv(os.path.join(results_dir, \"gcn_regression_sweep.csv\"), index=False)\n",
        "print(\"Saved regression hyperparameter sweep\")\n",
        "\n",
        "# Step 5: Retrain Each Fold & Save CV Summary\n",
        "best = sweep_df.loc[sweep_df[\"mean_mse\"].idxmin()]\n",
        "best_hd, best_dp, best_lr = int(best.hidden_dim), float(best.dropout), float(best.lr)\n",
        "\n",
        "fold_metrics = []\n",
        "for fold in range(kfold):\n",
        "    tr_data = torch.load(\n",
        "        os.path.join(base_path, f\"{task}_train_fold{fold}.pt\"),\n",
        "        weights_only=False\n",
        "    )\n",
        "    vl_data = torch.load(\n",
        "        os.path.join(base_path, f\"{task}_val_fold{fold}.pt\"),\n",
        "        weights_only=False\n",
        "    )\n",
        "\n",
        "    tr_loader = DataLoader(\n",
        "        tr_data, batch_size=32, shuffle=True,\n",
        "        worker_init_fn=seed_worker, generator=generator\n",
        "    )\n",
        "    vl_loader = DataLoader(vl_data, batch_size=32)\n",
        "\n",
        "    model     = GCNReg(tr_data[0].x.size(1), best_hd, dropout=best_dp).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
        "    best_vloss, patience = float(\"inf\"), 0\n",
        "\n",
        "    for epoch in range(1, 101):\n",
        "        model.train()\n",
        "        for batch in tr_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out    = model(batch).view(-1)\n",
        "            target = batch.y.view(-1).float()\n",
        "            loss   = F.mse_loss(out, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # validation\n",
        "        preds, labels = evaluate(model, vl_loader)\n",
        "        vloss = mean_squared_error(labels.numpy(), preds.numpy())\n",
        "\n",
        "        if vloss < best_vloss:\n",
        "            best_vloss, patience = vloss, 0\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(results_dir, f\"fold{fold+1}_model.pt\")\n",
        "            )\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= 10:\n",
        "                break\n",
        "\n",
        "    # compute metrics on this fold\n",
        "    preds, labels = evaluate(model, vl_loader)\n",
        "    y_true = labels.numpy()\n",
        "    y_pred = preds.numpy()\n",
        "    mse    = mean_squared_error(y_true, y_pred)\n",
        "    mae    = mean_absolute_error(y_true, y_pred)\n",
        "    r2     = r2_score(y_true, y_pred)\n",
        "\n",
        "    fold_metrics.append({\n",
        "        \"fold\": fold+1,\n",
        "        \"mse\":   mse,\n",
        "        \"mae\":   mae,\n",
        "        \"r2\":    r2\n",
        "    })\n",
        "\n",
        "cv_df = pd.DataFrame(fold_metrics)\n",
        "cv_df.to_csv(\n",
        "    os.path.join(results_dir, \"gcn_regression_cv_summary.csv\"),\n",
        "    index=False\n",
        ")\n",
        "print(\"Saved regression CV summary\")\n",
        "\n",
        "# Step 6: Plot CV Results (MSE per fold)\n",
        "cv_df = pd.read_csv(os.path.join(results_dir, \"gcn_regression_cv_summary.csv\"))\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(cv_df[\"fold\"], cv_df[\"mse\"])\n",
        "plt.title(\"Fold-wise Validation MSE\"); plt.xlabel(\"Fold\"); plt.ylabel(\"MSE\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Ensemble Averaging on Test Set\n",
        "test_data   = torch.load(\n",
        "    os.path.join(base_path, f\"{task}_test.pt\"),\n",
        "    weights_only=False\n",
        ")\n",
        "test_loader = DataLoader(test_data, batch_size=32)\n",
        "all_preds   = []\n",
        "\n",
        "for fold in range(kfold):\n",
        "    model = GCNReg(\n",
        "        test_data[0].x.size(1), best_hd, dropout=best_dp\n",
        "    ).to(device)\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            os.path.join(results_dir, f\"fold{fold+1}_model.pt\"),\n",
        "            weights_only=False\n",
        "        )\n",
        "    )\n",
        "    model.eval()\n",
        "    preds_fold = []\n",
        "    with torch.no_grad():\n",
        "        for b in test_loader:\n",
        "            preds_fold.append(model(b.to(device)).cpu().view(-1))\n",
        "    all_preds.append(torch.cat(preds_fold))\n",
        "\n",
        "avg_preds = torch.stack(all_preds).mean(0)\n",
        "y_true    = torch.cat([d.y for d in test_data]).view(-1).numpy()\n",
        "y_pred    = avg_preds.numpy()\n",
        "\n",
        "# Ensemble metrics\n",
        "mse_e = mean_squared_error(y_true, y_pred)\n",
        "mae_e = mean_absolute_error(y_true, y_pred)\n",
        "r2_e  = r2_score(y_true, y_pred)\n",
        "print(f\"Ensemble Test → MSE: {mse_e:.4f}, MAE: {mae_e:.4f}, R²: {r2_e:.4f}\")\n",
        "\n",
        "# Step 8: Final Training on Combined Data & Test Eval\n",
        "all_data = []\n",
        "for f in range(kfold):\n",
        "    all_data += torch.load(\n",
        "        os.path.join(base_path, f\"{task}_train_fold{f}.pt\"),\n",
        "        weights_only=False\n",
        "    )\n",
        "    all_data += torch.load(\n",
        "        os.path.join(base_path, f\"{task}_val_fold{f}.pt\"),\n",
        "        weights_only=False\n",
        "    )\n",
        "\n",
        "labels = [float(d.y.item()) for d in all_data]\n",
        "sss    = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=seed)\n",
        "tr_idx, vl_idx = next(sss.split(all_data, labels))\n",
        "train_split = [all_data[i] for i in tr_idx]\n",
        "val_split   = [all_data[i] for i in vl_idx]\n",
        "\n",
        "tr_loader = DataLoader(train_split, batch_size=32, shuffle=True,\n",
        "                       worker_init_fn=seed_worker, generator=generator)\n",
        "vl_loader = DataLoader(val_split,   batch_size=32)\n",
        "\n",
        "model     = GCNReg(all_data[0].x.size(1), best_hd, dropout=best_dp).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", patience=5, factor=0.5, verbose=True\n",
        ")\n",
        "\n",
        "best_vloss, patience = float(\"inf\"), 0\n",
        "for epoch in range(1, 301):\n",
        "    model.train()\n",
        "    tot_loss = 0\n",
        "    for b in tr_loader:\n",
        "        b = b.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out    = model(b).view(-1)\n",
        "        target = b.y.view(-1).float()\n",
        "        loss   = F.mse_loss(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "    preds, labels = evaluate(model, vl_loader)\n",
        "    vloss = mean_squared_error(labels.numpy(), preds.numpy())\n",
        "    scheduler.step(vloss)\n",
        "\n",
        "    if vloss < best_vloss:\n",
        "        best_vloss, patience = vloss, 0\n",
        "        torch.save(model.state_dict(), os.path.join(results_dir, \"gcn_final_model.pt\"))\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= 10:\n",
        "            break\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} | TrainLoss {tot_loss/len(tr_loader):.4f} | ValMSE {vloss:.4f}\")\n",
        "\n",
        "# Final test evaluation\n",
        "model.load_state_dict(torch.load(os.path.join(results_dir, \"gcn_final_model.pt\"), weights_only=False))\n",
        "test_loader = DataLoader(\n",
        "    torch.load(os.path.join(base_path, f\"{task}_test.pt\"), weights_only=False),\n",
        "    batch_size=32\n",
        ")\n",
        "preds, labels = evaluate(model, test_loader)\n",
        "y_true = labels.numpy()\n",
        "y_pred = preds.numpy()\n",
        "\n",
        "mse_f = mean_squared_error(y_true, y_pred)\n",
        "mae_f = mean_absolute_error(y_true, y_pred)\n",
        "r2_f  = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Final Model Test → MSE: {mse_f:.4f}, MAE: {mae_f:.4f}, R²: {r2_f:.4f}\")\n",
        "\n",
        "# Step 9: Baseline Comparison\n",
        "# baseline = pd.read_csv(os.path.join(root_dir, \"6_baseline_regression.csv\"))\n",
        "# comp = pd.DataFrame({\n",
        "#   \"GCN_Final\": [mse_f],\n",
        "#   \"Baseline\":  [baseline[\"mse\"].iloc[0]]\n",
        "# }, index=[\"MSE\"])\n",
        "# display(comp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MrLNVlAZTnCc",
        "outputId": "e705f2a4-06d2-4271-e79c-ceedc7783bd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "hd=64 dp=0.0 lr=0.001 → MSE 1.1632 ± 0.1185\n",
            "hd=64 dp=0.0 lr=0.0005 → MSE 1.2192 ± 0.1148\n",
            "hd=64 dp=0.2 lr=0.001 → MSE 1.1464 ± 0.1081\n",
            "hd=64 dp=0.2 lr=0.0005 → MSE 1.2018 ± 0.0931\n",
            "hd=128 dp=0.0 lr=0.001 → MSE 1.1465 ± 0.1561\n",
            "hd=128 dp=0.0 lr=0.0005 → MSE 1.1880 ± 0.1221\n",
            "hd=128 dp=0.2 lr=0.001 → MSE 1.1126 ± 0.1056\n",
            "hd=128 dp=0.2 lr=0.0005 → MSE 1.1572 ± 0.1133\n",
            "✅ Saved regression hyperparameter sweep\n",
            "✅ Saved regression CV summary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAGJCAYAAACZ7rtNAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM4RJREFUeJzt3XlcVdX+//H3AeSAoqghIIZiluFQTiihkVooIVHW7Zs5JJLZoJbKbZBK0LKobpoNKGkpdcuhSSvH1ERLKQektMwGNYkEtRIUDRL2749+nusJUFBhs/X1fDz2H3vttc7+bI7W28U669gMwzAEAAAAWJCL2QUAAAAAZ4owCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswC8BUaWlpstls2rNnz2n7BgUFadiwYdVWS69evdSrV69qe/2aMGzYMAUFBTm12Ww2TZw48bRjJ06cKJvNdk7rSU9Pl81mU3p6+jl9XQA4gTALoMpOBNDyjvHjx5tdniVkZmbKZrPp8ccfr7DPDz/8IJvNpvj4+Bqs7MxMnz5daWlpZpfhpFevXrLZbLrsssvKvb5y5UrHn9v33nvP6dq2bdt06623qkWLFvLw8FCzZs3Up08fvfzyy079goKCKvy7cP3111fbswH4HzezCwBgXU888YRatmzp1Na+fXuTqjl7n3zySY3dq3PnzgoODta8efM0efLkcvvMnTtXkjRkyJCzutexY8fk5la9/7mfPn26fHx8ysycX3PNNTp27Jjc3d2r9f4V8fDw0I8//qiNGzeqW7duTtfefvtteXh46M8//3Rq37Bhg3r37q3mzZtrxIgR8vf3V3Z2tr744gu9+OKLuv/++536d+zYUf/+97/L3DsgIODcPxCAMgizAM5YVFSUQkJCzC7jnKnpwDV48GBNmDBBX3zxha666qoy1+fNm6fg4GB17tz5rO7j4eFxVuPPhouLi6n3b9WqlY4fP6558+Y5hdk///xTCxcuVHR0tN5//32nMU899ZS8vb21adMmNWzY0Ona/v37y9yjWbNmZ/0PDgBnjmUGAKrNp59+qvDwcNWrV08NGzbUTTfdpB07dpx2nGEYmjx5si6++GLVrVtXvXv31jfffFOpe3799dey2Wz66KOPHG1btmyRzWYrEwqjoqIUGhrqOC9vzezLL7+sdu3aqW7dumrUqJFCQkIcM6Yn5OTk6M4775Sfn5/sdrvatWun2bNnn7bWwYMHS1KZ1ztR886dOx19PvzwQ0VHRysgIEB2u12tWrXSk08+qZKSktPep7w1s59//rm6du0qDw8PtWrVSq+++mq5Y+fMmaNrr71Wvr6+stvtatu2rWbMmOHUJygoSN98843Wrl3r+BX7iZ9jRWtm3333XXXp0kWenp7y8fHRkCFDlJOT49Rn2LBh8vLyUk5Ojvr37y8vLy81adJEDz74YKWe+4SBAwdqwYIFKi0tdbR9/PHHOnr0qG677bYy/X/66Se1a9euTJCVJF9f30rfF0DNYGYWwBnLz8/XwYMHndp8fHwkSatWrVJUVJQuueQSTZw4UceOHdPLL7+sHj16KDMzs8yHlE6WmJioyZMnq1+/furXr58yMzPVt29fFRcXn7am9u3bq2HDhlq3bp1uvPFGSdJnn30mFxcXffXVVyooKFCDBg1UWlqqDRs26O67767wtWbNmqUHHnhAt956q8aMGaM///xTX3/9tb788ksNGjRIkpSXl6errrpKNptNo0ePVpMmTbRs2TINHz5cBQUFGjt2bIWv37JlS3Xv3l3vvPOOXnjhBbm6ujqunQi4J+6TlpYmLy8vxcfHy8vLS59++qkSExNVUFCg//znP6f9uZxs27Zt6tu3r5o0aaKJEyfq+PHjSkpKkp+fX5m+M2bMULt27XTjjTfKzc1NH3/8sUaOHKnS0lKNGjVKkjRt2jTdf//98vLy0mOPPSZJ5b7WCWlpaYqLi1PXrl2VnJysvLw8vfjii1q/fr22bt3qFCJLSkoUGRmp0NBQPf/881q1apWmTJmiVq1a6b777qvU8w4aNEgTJ05Uenq6rr32Wkl//3yvu+66csNpixYtlJGRoe3bt1dq2cxff/1V5u+BJNWrV0+enp6VqhHAWTAAoIrmzJljSCr3OKFjx46Gr6+v8dtvvznavvrqK8PFxcUYOnRomdfavXu3YRiGsX//fsPd3d2Ijo42SktLHf0effRRQ5IRGxt72vqio6ONbt26Oc5vueUW45ZbbjFcXV2NZcuWGYZhGJmZmYYk48MPP3T069mzp9GzZ0/H+U033WS0a9fulPcaPny40bRpU+PgwYNO7bfffrvh7e1tHD169JTjU1JSDEnGihUrHG0lJSVGs2bNjLCwMEdbea9zzz33GHXr1jX+/PNPR1tsbKzRokULp36SjKSkJMd5//79DQ8PD+Pnn392tH377beGq6ur8c//LZR338jISOOSSy5xamvXrp3Tz+6ENWvWGJKMNWvWGIZhGMXFxYavr6/Rvn1749ixY45+ixcvNiQZiYmJTs8iyXjiiSecXrNTp05Gly5dytzrn3r27Ol4/0JCQozhw4cbhmEYf/zxh+Hu7m688cYbjvreffddx7hPPvnEcHV1NVxdXY2wsDDj4YcfNlasWGEUFxeXuUeLFi0q/LuQnJx82hoBnD2WGQA4YykpKVq5cqXTIUn79u1TVlaWhg0bpsaNGzv6X3nllerTp4+WLl1a4WuuWrVKxcXFuv/++522iTrVDOc/hYeHKzMzU4WFhZL+/pV6v3791LFjR3322WeS/p6ttdlsuvrqqyt8nYYNG+qXX37Rpk2byr1uGIbef/99xcTEyDAMHTx40HFERkYqPz9fmZmZp6x1wIABqlOnjtNSg7Vr1yonJ8exxECS0wzf4cOHdfDgQYWHh+vo0aP67rvvTv9D+f9KSkq0YsUK9e/fX82bN3e0t2nTRpGRkWX6n3zfEzPxPXv21K5du5Sfn1/p+56wefNm7d+/XyNHjnRaSxsdHa3g4GAtWbKkzJh7773X6Tw8PFy7du2q0n0HDRqkDz74QMXFxXrvvffk6uqqm2++udy+ffr0UUZGhm688UZ99dVXeu655xQZGalmzZo5LV85ITQ0tMzfg5UrV2rgwIFVqhHAmWGZAYAz1q1bt3I/APbzzz9Lki6//PIy19q0aaMVK1aosLBQ9erVq3DsP7dTatKkiRo1auQ4Lykp0YEDB5z6NG7cWO7u7goPD9fx48eVkZGhwMBA7d+/X+Hh4frmm2+cwmzbtm2dwvY/PfLII1q1apW6deumSy+9VH379tWgQYPUo0cPSdKBAwd06NAhzZw5UzNnziz3Ncr7wNDJLrroIkVGRmrhwoVKTU2Vh4eH5s6dKzc3N6f1nN98840ef/xxffrppyooKHB6jaqEygMHDujYsWPlbld1+eWXl/mHxvr165WUlKSMjAwdPXq0zH29vb0rfW/p1H82goOD9fnnnzu1eXh4qEmTJk5tjRo10h9//FGl+95+++168MEHtWzZMr399tu64YYbVL9+/Qr7d+3a1RF+v/rqKy1cuFAvvPCCbr31VmVlZalt27aOvj4+PoqIiKhSPQDOHcIsAEvKzs4usy3YmjVr1KtXL4WEhMjDw0Pr1q1T8+bN5evrq9atWys8PFzTp09XUVGRPvvsswpn5k5o06aNdu7cqcWLF2v58uV6//33NX36dCUmJmrSpEmODxQNGTJEsbGx5b7GlVdeedpnGTJkiBYvXqzFixfrxhtv1Pvvv+9Y0ypJhw4dUs+ePdWgQQM98cQTatWqlTw8PJSZmalHHnnE6YNN59JPP/2k6667TsHBwZo6daoCAwPl7u6upUuX6oUXXqi2+57s5HXEZ6Np06bq1auXpkyZovXr15fZwaAi7u7u6tq1q7p27arWrVsrLi5O7777rpKSks5JXQDOHmEWwDnXokULSdLOnTvLXPvuu+/k4+NT7qzsyWN/+OEHXXLJJY72AwcOOM3G+fv7O5Y1nNChQwdJfweQbt266bPPPlPz5s0VHh4u6e9fTxcVFentt99WXl6errnmmtM+S7169TRgwAANGDBAxcXFuuWWW/TUU08pISFBTZo0Uf369VVSUnJWM3M33nij6tevr7lz56pOnTr6448/nJYYpKen67ffftMHH3zgVPPu3burfK8mTZrI09NTP/zwQ5lr/3y/Pv74YxUVFemjjz5yWpKwZs2aMmMr+81hJ//ZOPFhrJPvf+J6dRg0aJDuuusuNWzYUP369avy+BO/hdi3b9+5Lg3AWWDNLIBzrmnTpurYsaPeeOMNHTp0yNG+fft2ffLJJ6cMEhEREapTp45efvllGYbhaJ82bZpTPw8PD0VERDgdJy9DCA8P15dffqk1a9Y4wqyPj4/atGmjZ5991tHnVH777Tenc3d3d7Vt21aGYeivv/6Sq6ur/vWvf+n999/X9u3by4z/5zKIinh6eurmm2/W0qVLNWPGDNWrV0833XST4/qJ2cmTfx7FxcWaPn16pV7/ZK6uroqMjNSiRYu0d+9eR/uOHTu0YsWKMn3/ed/8/HzNmTOnzOvWq1fP6b2uSEhIiHx9fZWamqqioiJH+7Jly7Rjxw5FR0dX9ZEq7dZbb1VSUpKmT59+yj2F16xZ4/TMJ5xYglHeEgkA5mFmFkC1+M9//qOoqCiFhYVp+PDhjq25vL29y+x5erIT+4gmJyfrhhtuUL9+/bR161YtW7bMse1XZYSHh+upp55Sdna2U2i95ppr9OqrryooKEgXX3zxKV+jb9++8vf3V48ePeTn56cdO3bolVdeUXR0tGO95TPPPKM1a9YoNDRUI0aMUNu2bfX7778rMzNTq1at0u+//16peocMGaI333xTK1as0ODBg51mrrt3765GjRopNjZWDzzwgGw2m/773/+WG7gqY9KkSVq+fLnCw8M1cuRIHT9+3LGf7tdff+30/O7u7oqJidE999yjI0eOaNasWfL19S0zO9mlSxfNmDFDkydP1qWXXipfX98yM6+SVKdOHT377LOKi4tTz549NXDgQMfWXEFBQRo3btwZPVNlnO7P3gn333+/jh49qptvvlnBwcEqLi7Whg0btGDBAgUFBSkuLs6pf05Ojt56660yr+Pl5aX+/fufo+oBVMi8jRQAWNWJ7bQ2bdp0yn6rVq0yevToYXh6ehoNGjQwYmJijG+//bbc1zqxNZdh/L011aRJk4ymTZsanp6eRq9evYzt27cbLVq0qNTWXIZhGAUFBYarq6tRv3594/jx4472t956y5Bk3HHHHWXG/HNrrldffdW45pprjIsuusiw2+1Gq1atjIceesjIz893GpeXl2eMGjXKCAwMNOrUqWP4+/sb1113nTFz5sxK1WoYhnH8+HGjadOmhiRj6dKlZa6vX7/euOqqqwxPT08jICDAsV2UTtr2yjAqtzWXYRjG2rVrjS5duhju7u7GJZdcYqSmphpJSUlltub66KOPjCuvvNLw8PAwgoKCjGeffdaYPXt2mfcsNzfXiI6ONurXr29Icvwc/7k11wkLFiwwOnXqZNjtdqNx48bG4MGDjV9++cWpT2xsrFGvXr0yP4vy6izPyVtzVaS8rbmWLVtm3HnnnUZwcLDh5eVluLu7G5deeqlx//33G3l5eU7jT7U11z/fBwDVw2YYZ/hPewAAAMBkrJkFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFkX3JcmlJaW6tdff1X9+vUr/fWLAAAAqDmGYejw4cMKCAiQi8up514vuDD766+/KjAw0OwyAAAAcBrZ2dmn/bbGCy7MnvgKyuzsbDVo0MDkagAAAPBPBQUFCgwMdOS2U7ngwuyJpQUNGjQgzAIAANRilVkSygfAAAAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACW5WZ2AQAAABeSoPFLzC7hjOx5JtrsEsrFzCwAAAAsizALAAAAyyLMAgAAwLIIswAAALAsPgAGVAGL9gEAqF1MnZldt26dYmJiFBAQIJvNpkWLFlV67Pr16+Xm5qaOHTtWW30AAACo3UwNs4WFherQoYNSUlKqNO7QoUMaOnSorrvuumqqDAAAAFZg6jKDqKgoRUVFVXncvffeq0GDBsnV1bVKs7kAAAA4v1juA2Bz5szRrl27lJSUVKn+RUVFKigocDoAAABwfrBUmP3hhx80fvx4vfXWW3Jzq9ykcnJysry9vR1HYGBgNVcJAACAmmKZMFtSUqJBgwZp0qRJat26daXHJSQkKD8/33FkZ2dXY5UAAACoSZbZmuvw4cPavHmztm7dqtGjR0uSSktLZRiG3Nzc9Mknn+jaa68tM85ut8tut9d0uQAAAKgBlgmzDRo00LZt25zapk+frk8//VTvvfeeWrZsaVJlAAAAMIupYfbIkSP68ccfHee7d+9WVlaWGjdurObNmyshIUE5OTl688035eLiovbt2zuN9/X1lYeHR5l2AAAAXBhMDbObN29W7969Hefx8fGSpNjYWKWlpWnfvn3au3evWeUBAACgljM1zPbq1UuGYVR4PS0t7ZTjJ06cqIkTJ57bogAAAGAZltnNAAAAAPgnwiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAy3Izu4ALQdD4JWaXcEb2PBNtdgkAAACnxMwsAAAALIuZWQAAUGvw20xUFTOzAAAAsCzCLAAAACyLMAsAAADLIswCAADAsgizAAAAsCzCLAAAACyLMAsAAADLYp9ZABcs9rMEAOszdWZ23bp1iomJUUBAgGw2mxYtWnTK/h988IH69OmjJk2aqEGDBgoLC9OKFStqplgAAADUOqaG2cLCQnXo0EEpKSmV6r9u3Tr16dNHS5cu1ZYtW9S7d2/FxMRo69at1VwpAAAAaiNTlxlERUUpKiqq0v2nTZvmdP7000/rww8/1Mcff6xOnTqd4+oAAABQ21l6zWxpaakOHz6sxo0bV9inqKhIRUVFjvOCgoKaKA0AAAA1wNK7GTz//PM6cuSIbrvttgr7JCcny9vb23EEBgbWYIUAAACoTpYNs3PnztWkSZP0zjvvyNfXt8J+CQkJys/PdxzZ2dk1WCUAAACqkyWXGcyfP1933XWX3n33XUVERJyyr91ul91ur6HKAAAAUJMsNzM7b948xcXFad68eYqOZq9FAACAC5mpM7NHjhzRjz/+6DjfvXu3srKy1LhxYzVv3lwJCQnKycnRm2++KenvpQWxsbF68cUXFRoaqtzcXEmSp6envL29TXkGAAAAmMfUmdnNmzerU6dOjm214uPj1alTJyUmJkqS9u3bp7179zr6z5w5U8ePH9eoUaPUtGlTxzFmzBhT6gcAAIC5TJ2Z7dWrlwzDqPB6Wlqa03l6enr1FgQAAABLsdyaWQAAAOAEwiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAsU8PsunXrFBMTo4CAANlsNi1atOi0Y9LT09W5c2fZ7XZdeumlSktLq/Y6AQAAUDuZGmYLCwvVoUMHpaSkVKr/7t27FR0drd69eysrK0tjx47VXXfdpRUrVlRzpQAAAKiN3My8eVRUlKKioirdPzU1VS1bttSUKVMkSW3atNHnn3+uF154QZGRkdVVJgAAAGopS62ZzcjIUEREhFNbZGSkMjIyKhxTVFSkgoICpwMAAADnB0uF2dzcXPn5+Tm1+fn5qaCgQMeOHSt3THJysry9vR1HYGBgTZQKAACAGmCpMHsmEhISlJ+f7ziys7PNLgkAAADniKlrZqvK399feXl5Tm15eXlq0KCBPD09yx1jt9tlt9trojwAgEmCxi8xu4QzsueZaLNLACzPUjOzYWFhWr16tVPbypUrFRYWZlJFAAAAMJOpYfbIkSPKyspSVlaWpL+33srKytLevXsl/b1EYOjQoY7+9957r3bt2qWHH35Y3333naZPn6533nlH48aNM6N8AAAAmMzUMLt582Z16tRJnTp1kiTFx8erU6dOSkxMlCTt27fPEWwlqWXLllqyZIlWrlypDh06aMqUKXrttdfYlgsAAOACZeqa2V69eskwjAqvl/ftXr169dLWrVursSoAAABYhaXWzAIAAAAnI8wCAADAsiy1NRdqN7bGAQAANY2ZWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFl8AAwAzmN8MBPA+Y6ZWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACW5WZ2AQAA4PSCxi8xu4QzsueZaLNLwHmOmVkAAABYFmEWAAAAlkWYBQAAgGWZHmZTUlIUFBQkDw8PhYaGauPGjafsP23aNF1++eXy9PRUYGCgxo0bpz///LOGqgUAAEBtUqUw+9xzz+nYsWOO8/Xr16uoqMhxfvjwYY0cObLSr7dgwQLFx8crKSlJmZmZ6tChgyIjI7V///5y+8+dO1fjx49XUlKSduzYoddff10LFizQo48+WpXHAAAAwHmiSmE2ISFBhw8fdpxHRUUpJyfHcX706FG9+uqrlX69qVOnasSIEYqLi1Pbtm2VmpqqunXravbs2eX237Bhg3r06KFBgwYpKChIffv21cCBA087mwsAAIDzU5XCrGEYpzyviuLiYm3ZskURERH/K8bFRREREcrIyCh3TPfu3bVlyxZHeN21a5eWLl2qfv36VXifoqIiFRQUOB0AAAA4P5i2z+zBgwdVUlIiPz8/p3Y/Pz9999135Y4ZNGiQDh48qKuvvlqGYej48eO69957T7nMIDk5WZMmTTqntQMAAKB2MP0DYFWRnp6up59+WtOnT1dmZqY++OADLVmyRE8++WSFYxISEpSfn+84srOza7BiAAAAVKcqz8y+9tpr8vLykiQdP35caWlp8vHxkSSn9bSn4+PjI1dXV+Xl5Tm15+Xlyd/fv9wxEyZM0B133KG77rpLknTFFVeosLBQd999tx577DG5uJTN5na7XXa7vdJ1AQAAwDqqFGabN2+uWbNmOc79/f313//+t0yfynB3d1eXLl20evVq9e/fX5JUWlqq1atXa/To0eWOOXr0aJnA6urqKuns1u8CAADAmqoUZvfs2XNObx4fH6/Y2FiFhISoW7dumjZtmgoLCxUXFydJGjp0qJo1a6bk5GRJUkxMjKZOnapOnTopNDRUP/74oyZMmKCYmBhHqAUAAMCFw7QPgEnSgAEDdODAASUmJio3N1cdO3bU8uXLHR8K27t3r9NM7OOPPy6bzabHH39cOTk5atKkiWJiYvTUU0+Z9QgAAAAwUZXCbEZGhn777TfdcMMNjrY333xTSUlJKiwsVP/+/fXyyy9XaY3q6NGjK1xWkJ6e7lysm5uSkpKUlJRUlbIBAABwnqrSbgZPPPGEvvnmG8f5tm3bNHz4cEVERGj8+PH6+OOPHUsCAAAAgOpWpTCblZWl6667znE+f/58hYaGatasWYqPj9dLL72kd95555wXCQAAAJSnSmH2jz/+cPqSg7Vr1yoqKspx3rVrV/ZxBQAAQI2pUpj18/PT7t27Jf39dbSZmZm66qqrHNcPHz6sOnXqnNsKAQAAgApUKcz269dP48eP12effaaEhATVrVtX4eHhjutff/21WrVqdc6LBAAAAMpTpd0MnnzySd1yyy3q2bOnvLy8lJaWJnd3d8f12bNnq2/fvue8SAAAAKA8VQqzPj4+WrdunfLz8+Xl5VXmiwreffdd1a9f/5wWCAAAAFSkSmH2zjvvrFS/2bNnn1ExAAAAQFVUKcympaWpRYsW6tSpkwzDqK6aAJgsaPwSs0s4I3ueiTa7BABADatSmL3vvvs0b9487d69W3FxcRoyZIgaN25cXbUBAAAAp1Sl3QxSUlK0b98+Pfzww/r4448VGBio2267TStWrGCmFgAAADWuSmFWkux2uwYOHKiVK1fq22+/Vbt27TRy5EgFBQXpyJEj1VEjAAAAUK4qh1mnwS4ustlsMgxDJSUl56omAAAAoFKqHGaLioo0b9489enTR61bt9a2bdv0yiuvaO/evfLy8qqOGgEAAIByVekDYCNHjtT8+fMVGBioO++8U/PmzZOPj0911QYAAACcUpXCbGpqqpo3b65LLrlEa9eu1dq1a8vt98EHH5yT4gAAAIBTqVKYHTp0qGw2W3XVAgAAAFRJlb80AQAAAKgtzmo3AwAAAMBMhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlmV6mE1JSVFQUJA8PDwUGhqqjRs3nrL/oUOHNGrUKDVt2lR2u12tW7fW0qVLa6haAAAA1CZuZt58wYIFio+PV2pqqkJDQzVt2jRFRkZq586d8vX1LdO/uLhYffr0ka+vr9577z01a9ZMP//8sxo2bFjzxQMAAMB0pobZqVOnasSIEYqLi5MkpaamasmSJZo9e7bGjx9fpv/s2bP1+++/a8OGDapTp44kKSgoqCZLBgAAQC1i2jKD4uJibdmyRREREf8rxsVFERERysjIKHfMRx99pLCwMI0aNUp+fn5q3769nn76aZWUlFR4n6KiIhUUFDgdAAAAOD+YFmYPHjyokpIS+fn5ObX7+fkpNze33DG7du3Se++9p5KSEi1dulQTJkzQlClTNHny5Arvk5ycLG9vb8cRGBh4Tp8DAAAA5jH9A2BVUVpaKl9fX82cOVNdunTRgAED9Nhjjyk1NbXCMQkJCcrPz3cc2dnZNVgxAAAAqpNpa2Z9fHzk6uqqvLw8p/a8vDz5+/uXO6Zp06aqU6eOXF1dHW1t2rRRbm6uiouL5e7uXmaM3W6X3W4/t8UDAACgVjBtZtbd3V1dunTR6tWrHW2lpaVavXq1wsLCyh3To0cP/fjjjyotLXW0ff/992ratGm5QRYAAADnN1OXGcTHx2vWrFl64403tGPHDt13330qLCx07G4wdOhQJSQkOPrfd999+v333zVmzBh9//33WrJkiZ5++mmNGjXKrEcAAACAiUzdmmvAgAE6cOCAEhMTlZubq44dO2r58uWOD4Xt3btXLi7/y9uBgYFasWKFxo0bpyuvvFLNmjXTmDFj9Mgjj5j1CAAAADCRqWFWkkaPHq3Ro0eXey09Pb1MW1hYmL744otqrgoAAABWYKndDAAAAICTEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWbUizKakpCgoKEgeHh4KDQ3Vxo0bKzVu/vz5stls6t+/f/UWCAAAgFrJ9DC7YMECxcfHKykpSZmZmerQoYMiIyO1f//+U47bs2ePHnzwQYWHh9dQpQAAAKhtTA+zU6dO1YgRIxQXF6e2bdsqNTVVdevW1ezZsyscU1JSosGDB2vSpEm65JJLarBaAAAA1Camhtni4mJt2bJFERERjjYXFxdFREQoIyOjwnFPPPGEfH19NXz48NPeo6ioSAUFBU4HAAAAzg+mhtmDBw+qpKREfn5+Tu1+fn7Kzc0td8znn3+u119/XbNmzarUPZKTk+Xt7e04AgMDz7puAAAA1A6mLzOoisOHD+uOO+7QrFmz5OPjU6kxCQkJys/PdxzZ2dnVXCUAAABqipuZN/fx8ZGrq6vy8vKc2vPy8uTv71+m/08//aQ9e/YoJibG0VZaWipJcnNz086dO9WqVSunMXa7XXa7vRqqBwAAgNlMnZl1d3dXly5dtHr1akdbaWmpVq9erbCwsDL9g4ODtW3bNmVlZTmOG2+8Ub1791ZWVhZLCAAAAC4wps7MSlJ8fLxiY2MVEhKibt26adq0aSosLFRcXJwkaejQoWrWrJmSk5Pl4eGh9u3bO41v2LChJJVpBwAAwPnP9DA7YMAAHThwQImJicrNzVXHjh21fPlyx4fC9u7dKxcXSy3tBQAAQA0xPcxK0ujRozV69Ohyr6Wnp59ybFpa2rkvCAAAAJbAlCcAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsq1aE2ZSUFAUFBcnDw0OhoaHauHFjhX1nzZql8PBwNWrUSI0aNVJERMQp+wMAAOD8ZXqYXbBggeLj45WUlKTMzEx16NBBkZGR2r9/f7n909PTNXDgQK1Zs0YZGRkKDAxU3759lZOTU8OVAwAAwGymh9mpU6dqxIgRiouLU9u2bZWamqq6detq9uzZ5fZ/++23NXLkSHXs2FHBwcF67bXXVFpaqtWrV9dw5QAAADCbqWG2uLhYW7ZsUUREhKPNxcVFERERysjIqNRrHD16VH/99ZcaN25c7vWioiIVFBQ4HQAAADg/mBpmDx48qJKSEvn5+Tm1+/n5KTc3t1Kv8cgjjyggIMApEJ8sOTlZ3t7ejiMwMPCs6wYAAEDtYPoyg7PxzDPPaP78+Vq4cKE8PDzK7ZOQkKD8/HzHkZ2dXcNVAgAAoLq4mXlzHx8fubq6Ki8vz6k9Ly9P/v7+pxz7/PPP65lnntGqVat05ZVXVtjPbrfLbrefk3oBAABQu5g6M+vu7q4uXbo4fXjrxIe5wsLCKhz33HPP6cknn9Ty5csVEhJSE6UCAACgFjJ1ZlaS4uPjFRsbq5CQEHXr1k3Tpk1TYWGh4uLiJElDhw5Vs2bNlJycLEl69tlnlZiYqLlz5yooKMixttbLy0teXl6mPQcAAABqnulhdsCAATpw4IASExOVm5urjh07avny5Y4Phe3du1cuLv+bQJ4xY4aKi4t16623Or1OUlKSJk6cWJOlAwAAwGSmh1lJGj16tEaPHl3utfT0dKfzPXv2VH9BAAAAsARL72YAAACACxthFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZVK8JsSkqKgoKC5OHhodDQUG3cuPGU/d99910FBwfLw8NDV1xxhZYuXVpDlQIAAKA2MT3MLliwQPHx8UpKSlJmZqY6dOigyMhI7d+/v9z+GzZs0MCBAzV8+HBt3bpV/fv3V//+/bV9+/YarhwAAABmMz3MTp06VSNGjFBcXJzatm2r1NRU1a1bV7Nnzy63/4svvqjrr79eDz30kNq0aaMnn3xSnTt31iuvvFLDlQMAAMBsbmbevLi4WFu2bFFCQoKjzcXFRREREcrIyCh3TEZGhuLj453aIiMjtWjRonL7FxUVqaioyHGen58vSSooKDjL6iuvtOhojd3rXKrqz+hCeM4L4RklnrO2489sWRfCc14IzyhdGM95ITzjubqXYRin72yYKCcnx5BkbNiwwan9oYceMrp161bumDp16hhz5851aktJSTF8fX3L7Z+UlGRI4uDg4ODg4ODgsNiRnZ192jxp6sxsTUhISHCayS0tLdXvv/+uiy66SDabzcTKUFkFBQUKDAxUdna2GjRoYHY5OAu8l+cP3svzB+/l+eN8ei8Nw9Dhw4cVEBBw2r6mhlkfHx+5uroqLy/PqT0vL0/+/v7ljvH3969Sf7vdLrvd7tTWsGHDMy8apmnQoIHl/3Lib7yX5w/ey/MH7+X543x5L729vSvVz9QPgLm7u6tLly5avXq1o620tFSrV69WWFhYuWPCwsKc+kvSypUrK+wPAACA85fpywzi4+MVGxurkJAQdevWTdOmTVNhYaHi4uIkSUOHDlWzZs2UnJwsSRozZox69uypKVOmKDo6WvPnz9fmzZs1c+ZMMx8DAAAAJjA9zA4YMEAHDhxQYmKicnNz1bFjRy1fvlx+fn6SpL1798rF5X8TyN27d9fcuXP1+OOP69FHH9Vll12mRYsWqX379mY9AqqZ3W5XUlJSmeUisB7ey/MH7+X5g/fy/HGhvpc2w6jMngcAAABA7WP6lyYAAAAAZ4owCwAAAMsizAIAAMCyCLMAAACwLMIsaq3k5GR17dpV9evXl6+vr/r376+dO3eaXRbO0jPPPCObzaaxY8eaXQrOUE5OjoYMGaKLLrpInp6euuKKK7R582azy0IVlZSUaMKECWrZsqU8PT3VqlUrPfnkk+Jz4bXfunXrFBMTo4CAANlsNi1atMjpumEYSkxMVNOmTeXp6amIiAj98MMP5hRbAwizqLXWrl2rUaNG6YsvvtDKlSv1119/qW/fviosLDS7NJyhTZs26dVXX9WVV15pdik4Q3/88Yd69OihOnXqaNmyZfr22281ZcoUNWrUyOzSUEXPPvusZsyYoVdeeUU7duzQs88+q+eee04vv/yy2aXhNAoLC9WhQwelpKSUe/25557TSy+9pNTUVH355ZeqV6+eIiMj9eeff9ZwpTWDrblgGQcOHJCvr6/Wrl2ra665xuxyUEVHjhxR586dNX36dE2ePFkdO3bUtGnTzC4LVTR+/HitX79en332mdml4CzdcMMN8vPz0+uvv+5o+9e//iVPT0+99dZbJlaGqrDZbFq4cKH69+8v6e9Z2YCAAP373//Wgw8+KEnKz8+Xn5+f0tLSdPvtt5tYbfVgZhaWkZ+fL0lq3LixyZXgTIwaNUrR0dGKiIgwuxSchY8++kghISH6v//7P/n6+qpTp06aNWuW2WXhDHTv3l2rV6/W999/L0n66quv9PnnnysqKsrkynA2du/erdzcXKf/1np7eys0NFQZGRkmVlZ9TP8GMKAySktLNXbsWPXo0YNve7Og+fPnKzMzU5s2bTK7FJylXbt2acaMGYqPj9ejjz6qTZs26YEHHpC7u7tiY2PNLg9VMH78eBUUFCg4OFiurq4qKSnRU089pcGDB5tdGs5Cbm6uJDm+SfUEPz8/x7XzDWEWljBq1Cht375dn3/+udmloIqys7M1ZswYrVy5Uh4eHmaXg7NUWlqqkJAQPf3005KkTp06afv27UpNTSXMWsw777yjt99+W3PnzlW7du2UlZWlsWPHKiAggPcSlsIyA9R6o0eP1uLFi7VmzRpdfPHFZpeDKtqyZYv279+vzp07y83NTW5ublq7dq1eeuklubm5qaSkxOwSUQVNmzZV27ZtndratGmjvXv3mlQRztRDDz2k8ePH6/bbb9cVV1yhO+64Q+PGjVNycrLZpeEs+Pv7S5Ly8vKc2vPy8hzXzjeEWdRahmFo9OjRWrhwoT799FO1bNnS7JJwBq677jpt27ZNWVlZjiMkJESDBw9WVlaWXF1dzS4RVdCjR48yW+R9//33atGihUkV4UwdPXpULi7OMcDV1VWlpaUmVYRzoWXLlvL399fq1asdbQUFBfryyy8VFhZmYmXVh2UGqLVGjRqluXPn6sMPP1T9+vUda328vb3l6elpcnWorPr165dZ51yvXj1ddNFFrH+2oHHjxql79+56+umnddttt2njxo2aOXOmZs6caXZpqKKYmBg99dRTat68udq1a6etW7dq6tSpuvPOO80uDadx5MgR/fjjj47z3bt3KysrS40bN1bz5s01duxYTZ48WZdddplatmypCRMmKCAgwLHjwfmGrblQa9lstnLb58yZo2HDhtVsMTinevXqxdZcFrZ48WIlJCTohx9+UMuWLRUfH68RI0aYXRaq6PDhw5owYYIWLlyo/fv3KyAgQAMHDlRiYqLc3d3NLg+nkJ6ert69e5dpj42NVVpamgzDUFJSkmbOnKlDhw7p6quv1vTp09W6dWsTqq1+hFkAAABYFmtmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAeA80qtXL40dO/aUfYKCgvj2NQDnDcIsANQyw4YNk81mK3Oc/F3sAIC/uZldAACgrOuvv15z5sxxamvSpIlJ1QBA7cXMLADUQna7Xf7+/k6Hq6ur1q5dq27duslut6tp06YaP368jh8/XuHr7N+/XzExMfL09FTLli319ttv1+BTAED1Y2YWACwiJydH/fr107Bhw/Tmm2/qu+++04gRI+Th4aGJEyeWO2bYsGH69ddftWbNGtWpU0cPPPCA9u/fX7OFA0A1IswCQC20ePFieXl5Oc6joqLUunVrBQYG6pVXXpHNZlNwcLB+/fVXPfLII0pMTJSLi/Mv277//nstW7ZMGzduVNeuXSVJr7/+utq0aVOjzwIA1YkwCwC1UO/evTVjxgzHeb169TRq1CiFhYXJZrM52nv06KEjR47ol19+UfPmzZ1eY8eOHXJzc1OXLl0cbcHBwWrYsGG11w8ANYUwCwC1UL169XTppZeaXQYA1Hp8AAwALKJNmzbKyMiQYRiOtvXr16t+/fq6+OKLy/QPDg7W8ePHtWXLFkfbzp07dejQoZooFwBqBGEWACxi5MiRys7O1v3336/vvvtOH374oZKSkhQfH19mvawkXX755br++ut1zz336Msvv9SWLVt01113ydPT04TqAaB6EGYBwCKaNWumpUuXauPGjerQoYPuvfdeDR8+XI8//niFY+bMmaOAgAD17NlTt9xyi+6++275+vrWYNUAUL1sxsm/rwIAAAAshJlZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBl/T8GI9swymAfLgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Test → MSE: 1.0819, MAE: 0.8285, R²: 0.1857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | TrainLoss 5.0637 | ValMSE 1.1976\n",
            "Epoch 002 | TrainLoss 1.2383 | ValMSE 1.2405\n",
            "Epoch 003 | TrainLoss 1.2067 | ValMSE 1.2118\n",
            "Epoch 004 | TrainLoss 1.1871 | ValMSE 1.1154\n",
            "Epoch 005 | TrainLoss 1.1482 | ValMSE 1.0848\n",
            "Epoch 006 | TrainLoss 1.1023 | ValMSE 1.0325\n",
            "Epoch 007 | TrainLoss 1.0420 | ValMSE 1.0516\n",
            "Epoch 008 | TrainLoss 1.0006 | ValMSE 0.9585\n",
            "Epoch 009 | TrainLoss 0.9754 | ValMSE 0.9522\n",
            "Epoch 010 | TrainLoss 0.9430 | ValMSE 0.9120\n",
            "Epoch 011 | TrainLoss 0.9181 | ValMSE 0.9052\n",
            "Epoch 012 | TrainLoss 0.9372 | ValMSE 1.0779\n",
            "Epoch 013 | TrainLoss 0.9110 | ValMSE 0.8975\n",
            "Epoch 014 | TrainLoss 0.9102 | ValMSE 0.8890\n",
            "Epoch 015 | TrainLoss 0.8858 | ValMSE 0.8770\n",
            "Epoch 016 | TrainLoss 0.8920 | ValMSE 0.9418\n",
            "Epoch 017 | TrainLoss 0.8923 | ValMSE 0.9056\n",
            "Epoch 018 | TrainLoss 0.8820 | ValMSE 0.8776\n",
            "Epoch 019 | TrainLoss 0.8744 | ValMSE 1.0209\n",
            "Epoch 020 | TrainLoss 0.8786 | ValMSE 0.9127\n",
            "Epoch 021 | TrainLoss 0.8570 | ValMSE 0.8511\n",
            "Epoch 022 | TrainLoss 0.8703 | ValMSE 0.8531\n",
            "Epoch 023 | TrainLoss 0.8725 | ValMSE 0.8876\n",
            "Epoch 024 | TrainLoss 0.8546 | ValMSE 0.8648\n",
            "Epoch 025 | TrainLoss 0.8461 | ValMSE 0.8342\n",
            "Epoch 026 | TrainLoss 0.8345 | ValMSE 0.8248\n",
            "Epoch 027 | TrainLoss 0.8539 | ValMSE 0.8709\n",
            "Epoch 028 | TrainLoss 0.8419 | ValMSE 0.8317\n",
            "Epoch 029 | TrainLoss 0.8433 | ValMSE 0.8168\n",
            "Epoch 030 | TrainLoss 0.8416 | ValMSE 0.8738\n",
            "Epoch 031 | TrainLoss 0.8388 | ValMSE 0.8171\n",
            "Epoch 032 | TrainLoss 0.8248 | ValMSE 0.8346\n",
            "Epoch 033 | TrainLoss 0.8358 | ValMSE 0.8168\n",
            "Epoch 034 | TrainLoss 0.8116 | ValMSE 0.8864\n",
            "Epoch 035 | TrainLoss 0.8134 | ValMSE 0.8066\n",
            "Epoch 036 | TrainLoss 0.8180 | ValMSE 0.8066\n",
            "Epoch 037 | TrainLoss 0.8211 | ValMSE 0.8216\n",
            "Epoch 038 | TrainLoss 0.8024 | ValMSE 0.7910\n",
            "Epoch 039 | TrainLoss 0.7993 | ValMSE 0.7761\n",
            "Epoch 040 | TrainLoss 0.7948 | ValMSE 0.7763\n",
            "Epoch 041 | TrainLoss 0.8012 | ValMSE 0.7663\n",
            "Epoch 042 | TrainLoss 0.7831 | ValMSE 0.8187\n",
            "Epoch 043 | TrainLoss 0.8072 | ValMSE 0.7722\n",
            "Epoch 044 | TrainLoss 0.8163 | ValMSE 0.7604\n",
            "Epoch 045 | TrainLoss 0.7779 | ValMSE 0.7503\n",
            "Epoch 046 | TrainLoss 0.7893 | ValMSE 0.7500\n",
            "Epoch 047 | TrainLoss 0.7880 | ValMSE 0.7425\n",
            "Epoch 048 | TrainLoss 0.7688 | ValMSE 0.7464\n",
            "Epoch 049 | TrainLoss 0.7673 | ValMSE 0.7526\n",
            "Epoch 050 | TrainLoss 0.7828 | ValMSE 0.7426\n",
            "Epoch 051 | TrainLoss 0.7646 | ValMSE 0.7303\n",
            "Epoch 052 | TrainLoss 0.7550 | ValMSE 0.7783\n",
            "Epoch 053 | TrainLoss 0.7531 | ValMSE 0.7172\n",
            "Epoch 054 | TrainLoss 0.7640 | ValMSE 0.7313\n",
            "Epoch 055 | TrainLoss 0.7527 | ValMSE 0.7791\n",
            "Epoch 056 | TrainLoss 0.7610 | ValMSE 0.7543\n",
            "Epoch 057 | TrainLoss 0.7890 | ValMSE 0.7344\n",
            "Epoch 058 | TrainLoss 0.7698 | ValMSE 0.7122\n",
            "Epoch 059 | TrainLoss 0.7511 | ValMSE 0.9396\n",
            "Epoch 060 | TrainLoss 0.7434 | ValMSE 0.6983\n",
            "Epoch 061 | TrainLoss 0.7586 | ValMSE 0.8004\n",
            "Epoch 062 | TrainLoss 0.7346 | ValMSE 0.7043\n",
            "Epoch 063 | TrainLoss 0.7485 | ValMSE 0.7967\n",
            "Epoch 064 | TrainLoss 0.7427 | ValMSE 0.6931\n",
            "Epoch 065 | TrainLoss 0.7415 | ValMSE 0.7137\n",
            "Epoch 066 | TrainLoss 0.7205 | ValMSE 0.6882\n",
            "Epoch 067 | TrainLoss 0.7550 | ValMSE 0.6865\n",
            "Epoch 068 | TrainLoss 0.7150 | ValMSE 0.7239\n",
            "Epoch 069 | TrainLoss 0.7316 | ValMSE 0.6784\n",
            "Epoch 070 | TrainLoss 0.7288 | ValMSE 0.7191\n",
            "Epoch 071 | TrainLoss 0.7229 | ValMSE 0.6692\n",
            "Epoch 072 | TrainLoss 0.7246 | ValMSE 0.6684\n",
            "Epoch 073 | TrainLoss 0.7131 | ValMSE 0.6982\n",
            "Epoch 074 | TrainLoss 0.7422 | ValMSE 0.7448\n",
            "Epoch 075 | TrainLoss 0.7268 | ValMSE 0.6684\n",
            "Epoch 076 | TrainLoss 0.7045 | ValMSE 0.6673\n",
            "Epoch 077 | TrainLoss 0.7125 | ValMSE 0.6584\n",
            "Epoch 078 | TrainLoss 0.7136 | ValMSE 0.7461\n",
            "Epoch 079 | TrainLoss 0.7281 | ValMSE 0.7921\n",
            "Epoch 080 | TrainLoss 0.6995 | ValMSE 0.6651\n",
            "Epoch 081 | TrainLoss 0.6948 | ValMSE 0.6414\n",
            "Epoch 082 | TrainLoss 0.7071 | ValMSE 0.6445\n",
            "Epoch 083 | TrainLoss 0.7070 | ValMSE 0.6513\n",
            "Epoch 084 | TrainLoss 0.6985 | ValMSE 0.6276\n",
            "Epoch 085 | TrainLoss 0.6932 | ValMSE 0.6552\n",
            "Epoch 086 | TrainLoss 0.7154 | ValMSE 0.6387\n",
            "Epoch 087 | TrainLoss 0.6926 | ValMSE 0.6429\n",
            "Epoch 088 | TrainLoss 0.6805 | ValMSE 0.6266\n",
            "Epoch 089 | TrainLoss 0.6786 | ValMSE 0.6501\n",
            "Epoch 090 | TrainLoss 0.6940 | ValMSE 0.6264\n",
            "Epoch 091 | TrainLoss 0.6920 | ValMSE 0.6811\n",
            "Epoch 092 | TrainLoss 0.6906 | ValMSE 0.7388\n",
            "Epoch 093 | TrainLoss 0.6902 | ValMSE 0.6756\n",
            "Epoch 094 | TrainLoss 0.6778 | ValMSE 0.6294\n",
            "Epoch 095 | TrainLoss 0.6727 | ValMSE 0.7290\n",
            "Epoch 096 | TrainLoss 0.6809 | ValMSE 0.7219\n",
            "Epoch 097 | TrainLoss 0.6658 | ValMSE 0.6252\n",
            "Epoch 098 | TrainLoss 0.6498 | ValMSE 0.6026\n",
            "Epoch 099 | TrainLoss 0.6639 | ValMSE 0.6447\n",
            "Epoch 100 | TrainLoss 0.6708 | ValMSE 0.6233\n",
            "Epoch 101 | TrainLoss 0.6636 | ValMSE 0.5971\n",
            "Epoch 102 | TrainLoss 0.6486 | ValMSE 0.5975\n",
            "Epoch 103 | TrainLoss 0.6414 | ValMSE 0.5871\n",
            "Epoch 104 | TrainLoss 0.6520 | ValMSE 0.5869\n",
            "Epoch 105 | TrainLoss 0.6577 | ValMSE 0.6032\n",
            "Epoch 106 | TrainLoss 0.6415 | ValMSE 0.5938\n",
            "Epoch 107 | TrainLoss 0.6392 | ValMSE 0.5921\n",
            "Epoch 108 | TrainLoss 0.6346 | ValMSE 0.5965\n",
            "Epoch 109 | TrainLoss 0.6504 | ValMSE 0.5930\n",
            "Epoch 110 | TrainLoss 0.6431 | ValMSE 0.5814\n",
            "Epoch 111 | TrainLoss 0.6448 | ValMSE 0.5799\n",
            "Epoch 112 | TrainLoss 0.6314 | ValMSE 0.5918\n",
            "Epoch 113 | TrainLoss 0.6403 | ValMSE 0.5908\n",
            "Epoch 114 | TrainLoss 0.6319 | ValMSE 0.5756\n",
            "Epoch 115 | TrainLoss 0.6473 | ValMSE 0.5805\n",
            "Epoch 116 | TrainLoss 0.6358 | ValMSE 0.5822\n",
            "Epoch 117 | TrainLoss 0.6458 | ValMSE 0.5811\n",
            "Epoch 118 | TrainLoss 0.6291 | ValMSE 0.5956\n",
            "Epoch 119 | TrainLoss 0.6340 | ValMSE 0.6281\n",
            "Epoch 120 | TrainLoss 0.6449 | ValMSE 0.6038\n",
            "Epoch 121 | TrainLoss 0.6144 | ValMSE 0.5681\n",
            "Epoch 122 | TrainLoss 0.6166 | ValMSE 0.5687\n",
            "Epoch 123 | TrainLoss 0.6233 | ValMSE 0.5759\n",
            "Epoch 124 | TrainLoss 0.6283 | ValMSE 0.5864\n",
            "Epoch 125 | TrainLoss 0.6212 | ValMSE 0.5691\n",
            "Epoch 126 | TrainLoss 0.6124 | ValMSE 0.5927\n",
            "Epoch 127 | TrainLoss 0.6291 | ValMSE 0.5703\n",
            "Epoch 128 | TrainLoss 0.6159 | ValMSE 0.5581\n",
            "Epoch 129 | TrainLoss 0.6059 | ValMSE 0.5670\n",
            "Epoch 130 | TrainLoss 0.6140 | ValMSE 0.5793\n",
            "Epoch 131 | TrainLoss 0.6136 | ValMSE 0.5729\n",
            "Epoch 132 | TrainLoss 0.6264 | ValMSE 0.5589\n",
            "Epoch 133 | TrainLoss 0.6181 | ValMSE 0.5646\n",
            "Epoch 134 | TrainLoss 0.6133 | ValMSE 0.5672\n",
            "Epoch 135 | TrainLoss 0.6051 | ValMSE 0.5565\n",
            "Epoch 136 | TrainLoss 0.6032 | ValMSE 0.5576\n",
            "Epoch 137 | TrainLoss 0.6127 | ValMSE 0.5648\n",
            "Epoch 138 | TrainLoss 0.6007 | ValMSE 0.5560\n",
            "Epoch 139 | TrainLoss 0.6099 | ValMSE 0.5565\n",
            "Epoch 140 | TrainLoss 0.6112 | ValMSE 0.5629\n",
            "Epoch 141 | TrainLoss 0.6074 | ValMSE 0.5591\n",
            "Epoch 142 | TrainLoss 0.6096 | ValMSE 0.5545\n",
            "Epoch 143 | TrainLoss 0.6202 | ValMSE 0.5625\n",
            "Epoch 144 | TrainLoss 0.6101 | ValMSE 0.5544\n",
            "Epoch 145 | TrainLoss 0.6058 | ValMSE 0.5559\n",
            "Epoch 146 | TrainLoss 0.6081 | ValMSE 0.5540\n",
            "Epoch 147 | TrainLoss 0.6015 | ValMSE 0.5616\n",
            "Epoch 148 | TrainLoss 0.6109 | ValMSE 0.5539\n",
            "Epoch 149 | TrainLoss 0.6074 | ValMSE 0.5598\n",
            "Epoch 150 | TrainLoss 0.6085 | ValMSE 0.5536\n",
            "Epoch 151 | TrainLoss 0.6041 | ValMSE 0.5544\n",
            "Epoch 152 | TrainLoss 0.6001 | ValMSE 0.5522\n",
            "Epoch 153 | TrainLoss 0.6099 | ValMSE 0.5526\n",
            "Epoch 154 | TrainLoss 0.6046 | ValMSE 0.5553\n",
            "Epoch 155 | TrainLoss 0.5961 | ValMSE 0.5677\n",
            "Epoch 156 | TrainLoss 0.6045 | ValMSE 0.5521\n",
            "Epoch 157 | TrainLoss 0.6050 | ValMSE 0.5527\n",
            "Epoch 158 | TrainLoss 0.6048 | ValMSE 0.5541\n",
            "Epoch 159 | TrainLoss 0.5959 | ValMSE 0.5528\n",
            "Epoch 160 | TrainLoss 0.6034 | ValMSE 0.5534\n",
            "Epoch 161 | TrainLoss 0.6013 | ValMSE 0.5511\n",
            "Epoch 162 | TrainLoss 0.6012 | ValMSE 0.5562\n",
            "Epoch 163 | TrainLoss 0.6062 | ValMSE 0.5518\n",
            "Epoch 164 | TrainLoss 0.6038 | ValMSE 0.5541\n",
            "Epoch 165 | TrainLoss 0.5994 | ValMSE 0.5531\n",
            "Epoch 166 | TrainLoss 0.5971 | ValMSE 0.5504\n",
            "Epoch 167 | TrainLoss 0.6015 | ValMSE 0.5513\n",
            "Epoch 168 | TrainLoss 0.6020 | ValMSE 0.5505\n",
            "Epoch 169 | TrainLoss 0.6052 | ValMSE 0.5516\n",
            "Epoch 170 | TrainLoss 0.5918 | ValMSE 0.5515\n",
            "Epoch 171 | TrainLoss 0.5968 | ValMSE 0.5522\n",
            "Epoch 172 | TrainLoss 0.6068 | ValMSE 0.5504\n",
            "Epoch 173 | TrainLoss 0.5993 | ValMSE 0.5497\n",
            "Epoch 174 | TrainLoss 0.5988 | ValMSE 0.5507\n",
            "Epoch 175 | TrainLoss 0.5991 | ValMSE 0.5525\n",
            "Epoch 176 | TrainLoss 0.6007 | ValMSE 0.5496\n",
            "Epoch 177 | TrainLoss 0.5988 | ValMSE 0.5498\n",
            "Epoch 178 | TrainLoss 0.5902 | ValMSE 0.5493\n",
            "Epoch 179 | TrainLoss 0.5953 | ValMSE 0.5528\n",
            "Epoch 180 | TrainLoss 0.5950 | ValMSE 0.5497\n",
            "Epoch 181 | TrainLoss 0.6020 | ValMSE 0.5490\n",
            "Epoch 182 | TrainLoss 0.6043 | ValMSE 0.5506\n",
            "Epoch 183 | TrainLoss 0.5892 | ValMSE 0.5487\n",
            "Epoch 184 | TrainLoss 0.6033 | ValMSE 0.5506\n",
            "Epoch 185 | TrainLoss 0.6070 | ValMSE 0.5488\n",
            "Epoch 186 | TrainLoss 0.6040 | ValMSE 0.5502\n",
            "Epoch 187 | TrainLoss 0.6021 | ValMSE 0.5502\n",
            "Epoch 188 | TrainLoss 0.5991 | ValMSE 0.5496\n",
            "Epoch 189 | TrainLoss 0.6035 | ValMSE 0.5497\n",
            "Epoch 190 | TrainLoss 0.5960 | ValMSE 0.5486\n",
            "Epoch 191 | TrainLoss 0.5988 | ValMSE 0.5486\n",
            "Epoch 192 | TrainLoss 0.5935 | ValMSE 0.5495\n",
            "Epoch 193 | TrainLoss 0.6054 | ValMSE 0.5490\n",
            "Epoch 194 | TrainLoss 0.6047 | ValMSE 0.5485\n",
            "Epoch 195 | TrainLoss 0.5947 | ValMSE 0.5485\n",
            "Epoch 196 | TrainLoss 0.5934 | ValMSE 0.5487\n",
            "Epoch 197 | TrainLoss 0.6055 | ValMSE 0.5496\n",
            "Epoch 198 | TrainLoss 0.5934 | ValMSE 0.5503\n",
            "Epoch 199 | TrainLoss 0.6029 | ValMSE 0.5486\n",
            "Epoch 200 | TrainLoss 0.5986 | ValMSE 0.5496\n",
            "Epoch 201 | TrainLoss 0.6004 | ValMSE 0.5488\n",
            "Epoch 202 | TrainLoss 0.6010 | ValMSE 0.5486\n",
            "Epoch 203 | TrainLoss 0.5922 | ValMSE 0.5487\n",
            "Epoch 204 | TrainLoss 0.6014 | ValMSE 0.5485\n",
            "Epoch 205 | TrainLoss 0.6004 | ValMSE 0.5485\n",
            "Epoch 206 | TrainLoss 0.5908 | ValMSE 0.5493\n",
            "Epoch 207 | TrainLoss 0.5941 | ValMSE 0.5485\n",
            "Epoch 208 | TrainLoss 0.6014 | ValMSE 0.5485\n",
            "Epoch 209 | TrainLoss 0.6036 | ValMSE 0.5483\n",
            "Epoch 210 | TrainLoss 0.5992 | ValMSE 0.5485\n",
            "Epoch 211 | TrainLoss 0.5903 | ValMSE 0.5487\n",
            "Epoch 212 | TrainLoss 0.5955 | ValMSE 0.5487\n",
            "Epoch 213 | TrainLoss 0.5898 | ValMSE 0.5485\n",
            "Epoch 214 | TrainLoss 0.5992 | ValMSE 0.5487\n",
            "Epoch 215 | TrainLoss 0.5970 | ValMSE 0.5486\n",
            "Epoch 216 | TrainLoss 0.5963 | ValMSE 0.5485\n",
            "Epoch 217 | TrainLoss 0.5922 | ValMSE 0.5484\n",
            "Epoch 218 | TrainLoss 0.6039 | ValMSE 0.5485\n",
            "Final Model Test → MSE: 0.7486, MAE: 0.6824, R²: 0.4366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovpowBYlKDNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}