{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSS9Ks3BjIJV",
        "outputId": "14dfbd47-8b8b-4614-b317-2369f735ce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "##############for github\n",
        "#task = \"regression\"\n",
        "#train_data = torch.load(f\"../4_train_test_split/random_split/{task}/{task}_train.pt\")\n",
        "#val_data = torch.load(f\"../4_train_test_split/random_split/{task}/{task}_val.pt\")\n",
        "#test_data = torch.load(f\"../4_train_test_split/random_split/{task}/{task}_test.pt\")\n",
        "##############\n",
        "\n",
        "task = \"regression\"\n",
        "train_path = f\"/content/drive/MyDrive/GNN_model_TRPM8_Drug_Potency_prediction/4_train_test_split/random_split/{task}/{task}_train.pt\"\n",
        "val_path   = f\"/content/drive/MyDrive/GNN_model_TRPM8_Drug_Potency_prediction/4_train_test_split/random_split/{task}/{task}_val.pt\"\n",
        "test_path  = f\"/content/drive/MyDrive/GNN_model_TRPM8_Drug_Potency_prediction/4_train_test_split/random_split/{task}/{task}_test.pt\"\n",
        "\n",
        "print(\"Train file exists:\", os.path.exists(train_path))\n",
        "print(\"Val file exists:\", os.path.exists(val_path))\n",
        "print(\"Test file exists:\", os.path.exists(test_path))\n",
        "\n",
        "#load files with weights_only set to False\n",
        "train_data = torch.load(train_path, weights_only=False)\n",
        "val_data   = torch.load(val_path, weights_only=False)\n",
        "test_data  = torch.load(test_path, weights_only=False)\n",
        "\n",
        "print(\"Datasets loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kbny5hEjext",
        "outputId": "b3fec7fa-d454-490e-cd1e-bef1333c0445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train file exists: True\n",
            "Val file exists: True\n",
            "Test file exists: True\n",
            "Datasets loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "ZhTJhWj3jqLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        # Three GCN layers\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        # Two linear layers for regression (graph-level readout)\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        # Global pooling: aggregate node features to a graph-level embedding\n",
        "        x = global_mean_pool(x, batch)\n",
        "        # Apply the linear layers as a regressor\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------------\n",
        "#  Loss, Model, and Optimizer Setup\n",
        "# -------------------------------\n",
        "# For regression, our model's output channel is 1 and we use MSELoss\n",
        "model = GCN(in_channels=train_data[0].x.size(1), hidden_channels=64, out_channels=1)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# -------------------------------\n",
        "#  Training Loop\n",
        "# -------------------------------\n",
        "def train():\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        # For regression, targets are used directly\n",
        "        loss = criterion(out.squeeze(), batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation Function\n",
        "# -------------------------------\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            preds.append(out.squeeze().cpu())\n",
        "            labels.append(batch.y.cpu())\n",
        "    preds = torch.cat(preds)\n",
        "    labels = torch.cat(labels)\n",
        "    return mean_squared_error(labels, preds)\n",
        "\n",
        "# -------------------------------\n",
        "# Run Training for 200 Epochs\n",
        "# -------------------------------\n",
        "for epoch in range(1, 101):\n",
        "    train()\n",
        "    val_metric = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch:03d} - MSE: {val_metric:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Test Evaluation\n",
        "# -------------------------------\n",
        "test_metric = evaluate(test_loader)\n",
        "print(f\"\\nTest MSE: {test_metric:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK9YL0Zzj1dP",
        "outputId": "bf7fa209-e4a4-4c3d-c244-3d1637ef0748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 - MSE: 26.4517\n",
            "Epoch 002 - MSE: 1.3098\n",
            "Epoch 003 - MSE: 1.7961\n",
            "Epoch 004 - MSE: 1.1068\n",
            "Epoch 005 - MSE: 1.2503\n",
            "Epoch 006 - MSE: 1.1435\n",
            "Epoch 007 - MSE: 1.1295\n",
            "Epoch 008 - MSE: 1.3404\n",
            "Epoch 009 - MSE: 1.0770\n",
            "Epoch 010 - MSE: 1.0590\n",
            "Epoch 011 - MSE: 1.2139\n",
            "Epoch 012 - MSE: 1.1784\n",
            "Epoch 013 - MSE: 1.0905\n",
            "Epoch 014 - MSE: 1.2052\n",
            "Epoch 015 - MSE: 1.0224\n",
            "Epoch 016 - MSE: 1.2793\n",
            "Epoch 017 - MSE: 1.0116\n",
            "Epoch 018 - MSE: 1.2601\n",
            "Epoch 019 - MSE: 1.2248\n",
            "Epoch 020 - MSE: 1.0540\n",
            "Epoch 021 - MSE: 1.3698\n",
            "Epoch 022 - MSE: 1.1634\n",
            "Epoch 023 - MSE: 1.0792\n",
            "Epoch 024 - MSE: 1.0798\n",
            "Epoch 025 - MSE: 1.0940\n",
            "Epoch 026 - MSE: 1.3964\n",
            "Epoch 027 - MSE: 1.0773\n",
            "Epoch 028 - MSE: 1.0012\n",
            "Epoch 029 - MSE: 1.5103\n",
            "Epoch 030 - MSE: 1.2056\n",
            "Epoch 031 - MSE: 1.0913\n",
            "Epoch 032 - MSE: 1.5745\n",
            "Epoch 033 - MSE: 1.1977\n",
            "Epoch 034 - MSE: 1.0610\n",
            "Epoch 035 - MSE: 1.2037\n",
            "Epoch 036 - MSE: 1.1609\n",
            "Epoch 037 - MSE: 1.1636\n",
            "Epoch 038 - MSE: 1.1124\n",
            "Epoch 039 - MSE: 1.1581\n",
            "Epoch 040 - MSE: 1.2193\n",
            "Epoch 041 - MSE: 1.0101\n",
            "Epoch 042 - MSE: 1.4147\n",
            "Epoch 043 - MSE: 1.2010\n",
            "Epoch 044 - MSE: 1.2516\n",
            "Epoch 045 - MSE: 1.1696\n",
            "Epoch 046 - MSE: 1.0207\n",
            "Epoch 047 - MSE: 1.0269\n",
            "Epoch 048 - MSE: 1.5762\n",
            "Epoch 049 - MSE: 1.0462\n",
            "Epoch 050 - MSE: 1.1514\n",
            "Epoch 051 - MSE: 1.4501\n",
            "Epoch 052 - MSE: 1.0182\n",
            "Epoch 053 - MSE: 1.2357\n",
            "Epoch 054 - MSE: 1.2625\n",
            "Epoch 055 - MSE: 1.2362\n",
            "Epoch 056 - MSE: 0.9911\n",
            "Epoch 057 - MSE: 1.5225\n",
            "Epoch 058 - MSE: 1.2035\n",
            "Epoch 059 - MSE: 1.0036\n",
            "Epoch 060 - MSE: 1.1431\n",
            "Epoch 061 - MSE: 1.2251\n",
            "Epoch 062 - MSE: 1.4101\n",
            "Epoch 063 - MSE: 1.0569\n",
            "Epoch 064 - MSE: 1.1668\n",
            "Epoch 065 - MSE: 1.0452\n",
            "Epoch 066 - MSE: 0.9980\n",
            "Epoch 067 - MSE: 1.3710\n",
            "Epoch 068 - MSE: 1.0327\n",
            "Epoch 069 - MSE: 1.3509\n",
            "Epoch 070 - MSE: 0.9771\n",
            "Epoch 071 - MSE: 1.2750\n",
            "Epoch 072 - MSE: 1.0534\n",
            "Epoch 073 - MSE: 0.9720\n",
            "Epoch 074 - MSE: 1.0066\n",
            "Epoch 075 - MSE: 0.9731\n",
            "Epoch 076 - MSE: 1.1296\n",
            "Epoch 077 - MSE: 1.4984\n",
            "Epoch 078 - MSE: 1.0049\n",
            "Epoch 079 - MSE: 1.0019\n",
            "Epoch 080 - MSE: 1.4133\n",
            "Epoch 081 - MSE: 1.0058\n",
            "Epoch 082 - MSE: 1.1941\n",
            "Epoch 083 - MSE: 1.0759\n",
            "Epoch 084 - MSE: 1.0364\n",
            "Epoch 085 - MSE: 0.9691\n",
            "Epoch 086 - MSE: 1.3233\n",
            "Epoch 087 - MSE: 1.2496\n",
            "Epoch 088 - MSE: 1.0009\n",
            "Epoch 089 - MSE: 0.9864\n",
            "Epoch 090 - MSE: 1.3260\n",
            "Epoch 091 - MSE: 1.0041\n",
            "Epoch 092 - MSE: 0.9882\n",
            "Epoch 093 - MSE: 1.7075\n",
            "Epoch 094 - MSE: 0.9758\n",
            "Epoch 095 - MSE: 1.1832\n",
            "Epoch 096 - MSE: 1.0745\n",
            "Epoch 097 - MSE: 1.1151\n",
            "Epoch 098 - MSE: 1.0131\n",
            "Epoch 099 - MSE: 1.0131\n",
            "Epoch 100 - MSE: 1.0478\n",
            "\n",
            "Test MSE: 1.1613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        # Five GCN layers\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv4 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv5 = GCNConv(hidden_channels, hidden_channels)\n",
        "        # Two linear layers for regression on the graph-level embedding\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # Pass through the 5 GCN layers with ReLU activations\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = F.relu(self.conv4(x, edge_index))\n",
        "        x = F.relu(self.conv5(x, edge_index))\n",
        "\n",
        "        # Global mean pooling to obtain a graph-level representation\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # Fully connected layers for regression\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=0.3, training=self.training)  # Dropout rate set to 0.3\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "#  Loss, Model, and Optimizer Setup\n",
        "model = GCN(in_channels=train_data[0].x.size(1), hidden_channels=64, out_channels=1)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Learning rate set to 0.0005\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "def train():\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        # For regression, use the raw target values\n",
        "        loss = criterion(out.squeeze(), batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch)\n",
        "            preds.append(out.squeeze().cpu())\n",
        "            labels.append(batch.y.cpu())\n",
        "    preds = torch.cat(preds)\n",
        "    labels = torch.cat(labels)\n",
        "    return mean_squared_error(labels, preds)\n",
        "\n",
        "# Run Training for 100 Epochs\n",
        "for epoch in range(1, 101):\n",
        "    train()\n",
        "    val_mse = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch:03d} - MSE: {val_mse:.4f}\")\n",
        "\n",
        "# Final Test Evaluation\n",
        "test_mse = evaluate(test_loader)\n",
        "print(f\"\\nTest MSE: {test_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a9Fr0SRlOeh",
        "outputId": "fdd9ad73-c5e7-46b3-df8a-f13945599bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 - MSE: 47.4917\n",
            "Epoch 002 - MSE: 37.3044\n",
            "Epoch 003 - MSE: 10.9361\n",
            "Epoch 004 - MSE: 2.6307\n",
            "Epoch 005 - MSE: 1.6828\n",
            "Epoch 006 - MSE: 0.9838\n",
            "Epoch 007 - MSE: 1.0428\n",
            "Epoch 008 - MSE: 1.0782\n",
            "Epoch 009 - MSE: 1.0251\n",
            "Epoch 010 - MSE: 0.9895\n",
            "Epoch 011 - MSE: 1.1450\n",
            "Epoch 012 - MSE: 1.0444\n",
            "Epoch 013 - MSE: 1.0395\n",
            "Epoch 014 - MSE: 1.0253\n",
            "Epoch 015 - MSE: 1.0111\n",
            "Epoch 016 - MSE: 1.0520\n",
            "Epoch 017 - MSE: 1.0533\n",
            "Epoch 018 - MSE: 1.0188\n",
            "Epoch 019 - MSE: 1.0567\n",
            "Epoch 020 - MSE: 0.9690\n",
            "Epoch 021 - MSE: 1.0005\n",
            "Epoch 022 - MSE: 0.9678\n",
            "Epoch 023 - MSE: 0.9952\n",
            "Epoch 024 - MSE: 1.0071\n",
            "Epoch 025 - MSE: 1.1328\n",
            "Epoch 026 - MSE: 0.9723\n",
            "Epoch 027 - MSE: 1.0074\n",
            "Epoch 028 - MSE: 0.9770\n",
            "Epoch 029 - MSE: 1.0059\n",
            "Epoch 030 - MSE: 0.9826\n",
            "Epoch 031 - MSE: 0.9686\n",
            "Epoch 032 - MSE: 1.1417\n",
            "Epoch 033 - MSE: 1.0458\n",
            "Epoch 034 - MSE: 0.9730\n",
            "Epoch 035 - MSE: 1.0255\n",
            "Epoch 036 - MSE: 0.9689\n",
            "Epoch 037 - MSE: 1.0005\n",
            "Epoch 038 - MSE: 1.0713\n",
            "Epoch 039 - MSE: 1.0268\n",
            "Epoch 040 - MSE: 1.0107\n",
            "Epoch 041 - MSE: 1.4092\n",
            "Epoch 042 - MSE: 1.0369\n",
            "Epoch 043 - MSE: 0.9625\n",
            "Epoch 044 - MSE: 0.9926\n",
            "Epoch 045 - MSE: 1.0322\n",
            "Epoch 046 - MSE: 0.9789\n",
            "Epoch 047 - MSE: 0.9683\n",
            "Epoch 048 - MSE: 1.0932\n",
            "Epoch 049 - MSE: 0.9646\n",
            "Epoch 050 - MSE: 1.0154\n",
            "Epoch 051 - MSE: 0.9542\n",
            "Epoch 052 - MSE: 1.1054\n",
            "Epoch 053 - MSE: 0.9537\n",
            "Epoch 054 - MSE: 0.9926\n",
            "Epoch 055 - MSE: 1.1854\n",
            "Epoch 056 - MSE: 0.9602\n",
            "Epoch 057 - MSE: 0.9551\n",
            "Epoch 058 - MSE: 0.9813\n",
            "Epoch 059 - MSE: 1.1375\n",
            "Epoch 060 - MSE: 1.0417\n",
            "Epoch 061 - MSE: 1.0886\n",
            "Epoch 062 - MSE: 0.9514\n",
            "Epoch 063 - MSE: 0.9977\n",
            "Epoch 064 - MSE: 0.9436\n",
            "Epoch 065 - MSE: 1.0289\n",
            "Epoch 066 - MSE: 1.0076\n",
            "Epoch 067 - MSE: 1.1437\n",
            "Epoch 068 - MSE: 0.9756\n",
            "Epoch 069 - MSE: 0.9475\n",
            "Epoch 070 - MSE: 0.9393\n",
            "Epoch 071 - MSE: 1.1445\n",
            "Epoch 072 - MSE: 0.9719\n",
            "Epoch 073 - MSE: 0.9371\n",
            "Epoch 074 - MSE: 1.0748\n",
            "Epoch 075 - MSE: 1.0708\n",
            "Epoch 076 - MSE: 1.0548\n",
            "Epoch 077 - MSE: 0.9437\n",
            "Epoch 078 - MSE: 0.9574\n",
            "Epoch 079 - MSE: 0.9641\n",
            "Epoch 080 - MSE: 0.9238\n",
            "Epoch 081 - MSE: 0.9591\n",
            "Epoch 082 - MSE: 1.0135\n",
            "Epoch 083 - MSE: 0.9736\n",
            "Epoch 084 - MSE: 0.9484\n",
            "Epoch 085 - MSE: 0.9721\n",
            "Epoch 086 - MSE: 0.9302\n",
            "Epoch 087 - MSE: 1.2645\n",
            "Epoch 088 - MSE: 0.9974\n",
            "Epoch 089 - MSE: 1.0375\n",
            "Epoch 090 - MSE: 0.9145\n",
            "Epoch 091 - MSE: 0.9484\n",
            "Epoch 092 - MSE: 0.9331\n",
            "Epoch 093 - MSE: 1.7264\n",
            "Epoch 094 - MSE: 0.9429\n",
            "Epoch 095 - MSE: 0.9078\n",
            "Epoch 096 - MSE: 0.9708\n",
            "Epoch 097 - MSE: 0.9190\n",
            "Epoch 098 - MSE: 0.9041\n",
            "Epoch 099 - MSE: 1.1976\n",
            "Epoch 100 - MSE: 1.0958\n",
            "\n",
            "Test MSE: 1.1061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.histplot([data.y.item() for data in train_data])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "CtuNKWu_kAez",
        "outputId": "40bd0c95-e100-4134-bebc-f2c9757baa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PCCmUmQ_keoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}