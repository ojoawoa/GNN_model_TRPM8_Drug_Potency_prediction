{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518ef805",
   "metadata": {},
   "source": [
    "# Regression Training Pipeline\n",
    "\n",
    "This notebook implements a complete Graph Neural Network (GNN) pipeline for a 3-class classification task (`Low`, `Medium`, `High`).  It now includes additional metrics in hyperparameter tuning and enhanced model comparison visualizations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment & Setup](#step1)\n",
    "2. [Model Definition](#step2)\n",
    "3. [Evaluation Function](#step3)\n",
    "4. [Hyperparameter Sweep (10-Fold CV)](#step4)\n",
    "5. [Retraining & Validation (10-Fold CV)](#step5)\n",
    "6. [Cross-Validation Results Visualization](#step6)\n",
    "7. [Ensemble Averaging](#step7)\n",
    "8. [Final Model Training & Test Evaluation](#step8)\n",
    "9. [Baseline QSAR Comparison](#step9)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- PyTorch & PyTorch Geometric  \n",
    "- scikit-learn  \n",
    "- pandas, numpy, matplotlib  \n",
    "- RDKit (only for feature extraction)  \n",
    "- GPU recommended\n",
    "\n",
    "Install requirements:\n",
    "```bash\n",
    "pip install torch torch-geometric scikit-learn pandas numpy matplotlib rdkit-pypi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"step1\"></a>\n",
    "## Step 1: Environment & Setup\n",
    "- Seed all random generators for reproducibility.  \n",
    "- Define paths for data splits and output.  \n",
    "- Detect GPU/CPU.\n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "## Step 2: Model Definition\n",
    "Defines:\n",
    "- `MPNNLayer`: message-passing with dropout support.  \n",
    "- `MPNN`: stacks layers, global mean pool, and final linear head.\n",
    "\n",
    "<a id=\"step3\"></a>\n",
    "## Step 3: Evaluation Function\n",
    "`evaluate(model, loader)` returns concatenated logits and true labels.\n",
    "\n",
    "<a id=\"step4\"></a>\n",
    "## Step 4: Hyperparameter Sweep (10-Fold CV)\n",
    "**Updates:** Now tracks both **AUC-ROC** and **Balanced Accuracy** per fold.\n",
    "\n",
    "- Grid search over `hidden_dim`, `dropout`, `lr`.  \n",
    "- For each config, run 10-fold CV: train for 50 epochs, then evaluate validation set.\n",
    "- Compute per-fold metrics:\n",
    "  - **AUC-ROC** (one-vs-rest)\n",
    "  - **Balanced Accuracy** (accounts for class imbalance)\n",
    "- Record **mean Â± std** for both metrics.\n",
    "- Results DataFrame `sweep_df` now contains `mean_auc`, `std_auc`, `mean_balanced_acc`, and `std_balanced_acc`.\n",
    "\n",
    "<a id=\"step5\"></a>\n",
    "## Step 5: Retraining & Validation (10-Fold CV)\n",
    "- Retrain each fold with best hyperparameters and early stopping.  \n",
    "- Save best model weights.  \n",
    "- Compute per-fold classification metrics: accuracy, precision, recall, F1, AUC-ROC.  \n",
    "- Save `crossval_summary.csv`.\n",
    "\n",
    "<a id=\"step6\"></a>\n",
    "## Step 6: Cross-Validation Results Visualization\n",
    "- Load `crossval_summary.csv`.  \n",
    "- Plot bar charts for each metric across folds.  \n",
    "- Print mean Â± std.\n",
    "\n",
    "<a id=\"step7\"></a>\n",
    "## Step 7: Ensemble Averaging\n",
    "- Load fold checkpoints, run on test set, average logits.  \n",
    "- Save `ensemble_preds.csv` (True vs. Pred).\n",
    "- **Ensemble evaluation plots** include confusion matrix and per-class ROC curves.\n",
    "\n",
    "<a id=\"step8\"></a>\n",
    "## Step 8: Final Model Training & Test Evaluation\n",
    "- Merge all train+val folds, reserve 10% for validation.  \n",
    "- Train final model with early stopping and LR scheduler.  \n",
    "- Evaluate on hold-out test: accuracy, precision, recall, F1, AUC-ROC.\n",
    "- Plot confusion matrix and per-class ROC curves.  \n",
    "- Save `final_model_metrics.csv`, `final_confusion_matrix.png`, and `final_auc_roc.png`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters & Extensions\n",
    "- **Epochs:** 50 for CV, 100 for final training  \n",
    "- **Patience:** 10 for early stopping  \n",
    "- **LR Scheduler:** `ReduceLROnPlateau` on validation loss  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b034c",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe80c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout # For the multi-perceptron layer\n",
    "from torch_geometric.nn import GINConv  # Load graph isomorphism network\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42ec83",
   "metadata": {},
   "source": [
    "## 2. Task and Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a373c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "# DataLoader seeding\n",
    "from torch.utils.data import DataLoader as _DL\n",
    "from torch.utils.data import get_worker_info\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = seed + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(seed)\n",
    "\n",
    "#task = \"classification\"  # or \"regression\"\n",
    "task = \"regression\"  # or \"classification\"\n",
    "num_classes = 3\n",
    "class_names = {0: \"Low\", 1: \"Medium\", 2: \"High\"}\n",
    "kfold = 10\n",
    "base_path = f\"../4_train_test_split/10fold_cv/{task}/\"\n",
    "results_dir = f\"GIN_results/{task}_{kfold}fold/\"\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048a9fd",
   "metadata": {},
   "source": [
    "## 3. Define GIN Model with Dropout Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0736f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§± Define Model (GIN)\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout): # where \"dim_h\" is dimensionality of layer; number of features or neurons that layer processes\n",
    "        super(GIN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = GINConv( # design of the multilayer perceptron layer for the GINConv layer based on the paper (https://arxiv.org/pdf/1905.12265)\n",
    "            Sequential(Linear(in_channels, hidden_channels),\n",
    "                       BatchNorm1d(hidden_channels), ReLU(),\n",
    "                       Linear(hidden_channels, hidden_channels), ReLU()))\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(hidden_channels, hidden_channels), BatchNorm1d(hidden_channels), ReLU(),\n",
    "                       Linear(hidden_channels, hidden_channels), ReLU()))\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(Linear(hidden_channels, hidden_channels), BatchNorm1d(hidden_channels), ReLU(),\n",
    "                       Linear(hidden_channels, hidden_channels), ReLU()))\n",
    "        self.lin1 = Linear(hidden_channels*3, hidden_channels*3)\n",
    "        self.lin2 = Linear(hidden_channels*3, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # Node embeddings\n",
    "        h1 = self.conv1(x, edge_index) \n",
    "        h2 = self.conv2(h1, edge_index)\n",
    "        h3 = self.conv3(h2, edge_index)\n",
    "\n",
    "        # Graph level readout\n",
    "        h1 = global_add_pool(h1, batch) \n",
    "        h2 = global_add_pool(h2, batch)\n",
    "        h3 = global_add_pool(h3, batch)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        h = torch.cat((h1, h2, h3), dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fafbf",
   "metadata": {},
   "source": [
    "# ## Step 4: Select Best Hyperparameters\n",
    "# Use this section to manually define the best hyperparameters based on the sweep above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3512d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.0, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.1910 Â± 0.2655\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.0, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.3195 Â± 0.1988\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.0, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3448 Â± 0.1727\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.2, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.2120 Â± 0.1933\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.2, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2536 Â± 0.2837\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.2, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3033 Â± 0.1873\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.4, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.1777 Â± 0.1570\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.4, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2833 Â± 0.1591\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=64, dropout=0.4, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.2669 Â± 0.1837\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.0, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.2555 Â± 0.2665\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.0, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.3338 Â± 0.1284\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.0, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3103 Â± 0.2003\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.2, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.3596 Â± 0.2052\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.2, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2667 Â± 0.2256\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.2, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3745 Â± 0.1749\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.4, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.2112 Â± 0.1986\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.4, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2883 Â± 0.2345\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=128, dropout=0.4, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3681 Â± 0.1648\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.0, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.3806 Â± 0.1481\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.0, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.3102 Â± 0.2992\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.0, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3798 Â± 0.1229\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.2, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.2491 Â± 0.2898\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.2, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2823 Â± 0.1636\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.2, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3189 Â± 0.2586\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.4, lr=0.001\n",
      "ğŸ“Š RÂ²: 0.2762 Â± 0.2874\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.4, lr=0.0005\n",
      "ğŸ“Š RÂ²: 0.2818 Â± 0.1692\n",
      "\n",
      "ğŸ”§ Config: hidden_channels=256, dropout=0.4, lr=0.0001\n",
      "ğŸ“Š RÂ²: 0.3331 Â± 0.2124\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>dropout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>mean_r2</th>\n",
       "      <th>std_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.380562</td>\n",
       "      <td>0.148076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.379849</td>\n",
       "      <td>0.122935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.374518</td>\n",
       "      <td>0.174885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.368058</td>\n",
       "      <td>0.164819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.359608</td>\n",
       "      <td>0.205165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.344835</td>\n",
       "      <td>0.172729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.333842</td>\n",
       "      <td>0.128447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>256</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.333090</td>\n",
       "      <td>0.212395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.319516</td>\n",
       "      <td>0.198756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.318906</td>\n",
       "      <td>0.258591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.310297</td>\n",
       "      <td>0.200328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.310153</td>\n",
       "      <td>0.299227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.303312</td>\n",
       "      <td>0.187339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.234544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.283298</td>\n",
       "      <td>0.159101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.282273</td>\n",
       "      <td>0.163565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>256</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.281811</td>\n",
       "      <td>0.169225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>256</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.276220</td>\n",
       "      <td>0.287364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.266930</td>\n",
       "      <td>0.183741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.266716</td>\n",
       "      <td>0.225603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.255532</td>\n",
       "      <td>0.266549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.253600</td>\n",
       "      <td>0.283708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.249081</td>\n",
       "      <td>0.289844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.211984</td>\n",
       "      <td>0.193251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.211153</td>\n",
       "      <td>0.198561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.191009</td>\n",
       "      <td>0.265464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.177709</td>\n",
       "      <td>0.156957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_dim  dropout  learning_rate   mean_r2    std_r2\n",
       "18         256      0.0         0.0010  0.380562  0.148076\n",
       "20         256      0.0         0.0001  0.379849  0.122935\n",
       "14         128      0.2         0.0001  0.374518  0.174885\n",
       "17         128      0.4         0.0001  0.368058  0.164819\n",
       "12         128      0.2         0.0010  0.359608  0.205165\n",
       "2           64      0.0         0.0001  0.344835  0.172729\n",
       "10         128      0.0         0.0005  0.333842  0.128447\n",
       "26         256      0.4         0.0001  0.333090  0.212395\n",
       "1           64      0.0         0.0005  0.319516  0.198756\n",
       "23         256      0.2         0.0001  0.318906  0.258591\n",
       "11         128      0.0         0.0001  0.310297  0.200328\n",
       "19         256      0.0         0.0005  0.310153  0.299227\n",
       "5           64      0.2         0.0001  0.303312  0.187339\n",
       "16         128      0.4         0.0005  0.288300  0.234544\n",
       "7           64      0.4         0.0005  0.283298  0.159101\n",
       "22         256      0.2         0.0005  0.282273  0.163565\n",
       "25         256      0.4         0.0005  0.281811  0.169225\n",
       "24         256      0.4         0.0010  0.276220  0.287364\n",
       "8           64      0.4         0.0001  0.266930  0.183741\n",
       "13         128      0.2         0.0005  0.266716  0.225603\n",
       "9          128      0.0         0.0010  0.255532  0.266549\n",
       "4           64      0.2         0.0005  0.253600  0.283708\n",
       "21         256      0.2         0.0010  0.249081  0.289844\n",
       "3           64      0.2         0.0010  0.211984  0.193251\n",
       "15         128      0.4         0.0010  0.211153  0.198561\n",
       "0           64      0.0         0.0010  0.191009  0.265464\n",
       "6           64      0.4         0.0010  0.177709  0.156957"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "#10 fold cross-validation\n",
    "task = \"regression\"\n",
    "\n",
    "test_data = torch.load(f\"../4_train_test_split/10fold_cv/{task}/{task}_test.pt\")\n",
    "average_score = 0\n",
    "\n",
    "hidden_channels = [64, 128, 256]\n",
    "dropouts    = [0.0, 0.2, 0.4]\n",
    "lrs         = [1e-3, 5e-4, 1e-4]\n",
    "\n",
    "results = []\n",
    "\n",
    "for hd in hidden_channels:\n",
    "    for dp in dropouts:\n",
    "        for lr in lrs:\n",
    "            print(f\"\\nğŸ”§ Config: hidden_channels={hd}, dropout={dp}, lr={lr}\")\n",
    "            fold_metrics = []\n",
    "\n",
    "            for fold_idx in range(10):\n",
    "                #load data from fold_idx-th fold\n",
    "                train_data = torch.load(f\"../4_train_test_split/10fold_cv/{task}/{task}_train_fold{fold_idx}.pt\")\n",
    "                val_data = torch.load(f\"../4_train_test_split/10fold_cv/{task}/{task}_val_fold{fold_idx}.pt\")\n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                #initialize models\n",
    "                if task == \"classification\":\n",
    "                    num_classes = len(set([int(data.y.item()) for data in train_data]))\n",
    "                    model = GIN(in_channels=train_data[0].x.size(1), hidden_channels=hd, out_channels=num_classes, dropout=dp).to(device)\n",
    "                    criterion = torch.nn.CrossEntropyLoss()\n",
    "                else:\n",
    "                    model = GIN(in_channels=train_data[0].x.size(1), hidden_channels=hd, out_channels=1, dropout=dp).to(device)\n",
    "                    criterion = torch.nn.MSELoss()\n",
    "                \n",
    "                opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                # Initialize Data Loaders \n",
    "                tr = DataLoader(train_data, batch_size=32, shuffle=True,\n",
    "                                worker_init_fn=seed_worker, generator=generator)\n",
    "                vl = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "                # ğŸ“ˆ Evaluation\n",
    "                def evaluate(model, loader):\n",
    "                    model.eval()\n",
    "                    preds, labels = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for batch in loader:\n",
    "                            batch = batch.to(device)\n",
    "                            out = model(batch)\n",
    "                            preds.append(out.cpu())\n",
    "                            labels.append(batch.y.cpu())\n",
    "                    return torch.cat(preds), torch.cat(labels)\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(1, 51):\n",
    "                    model.train()\n",
    "                    for batch in tr:\n",
    "                        batch = batch.to(device)\n",
    "                        opt.zero_grad()\n",
    "                        out = model(batch)\n",
    "                        loss = F.mse_loss(out.squeeze(), batch.y)\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "                # Validation\n",
    "                preds, trues = evaluate(model, vl)\n",
    "                y_true = trues.numpy()\n",
    "                y_pred = preds.squeeze().numpy()\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "                fold_metrics.append(r2)\n",
    "\n",
    "            # Record mean and std\n",
    "            mean_r2 = np.mean(fold_metrics)\n",
    "            std_r2  = np.std(fold_metrics)\n",
    "            results.append((hd, dp, lr, mean_r2, std_r2))\n",
    "            print(f\"ğŸ“Š RÂ²: {mean_r2:.4f} Â± {std_r2:.4f}\")\n",
    "\n",
    "# Compile results into DataFrame\n",
    "sweep_df = pd.DataFrame(results,\n",
    "                        columns=[\"hidden_dim\", \"dropout\", \"learning_rate\", \"mean_r2\", \"std_r2\"])\n",
    "# Display sorted by best performance\n",
    "display(sweep_df.sort_values(\"mean_r2\", ascending=False))\n",
    "cv_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e702d",
   "metadata": {},
   "source": [
    "# ## Select best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456895e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hidden_dim = 256\n",
    "best_dropout = 0.0\n",
    "best_lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce99c49",
   "metadata": {},
   "source": [
    "# ## Step 5a: Retrain All Folds with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cf62689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Retraining Fold 1/10 with best hyperparameters\n",
      "Epoch 001 | Train Loss: 76.1625 | Val Loss: 3.4829\n",
      "Epoch 002 | Train Loss: 3.3861 | Val Loss: 2.0208\n",
      "Epoch 003 | Train Loss: 1.5070 | Val Loss: 1.2838\n",
      "Epoch 004 | Train Loss: 1.1418 | Val Loss: 1.0059\n",
      "Epoch 005 | Train Loss: 0.9706 | Val Loss: 0.8584\n",
      "Epoch 006 | Train Loss: 0.9688 | Val Loss: 0.8449\n",
      "Epoch 007 | Train Loss: 0.9035 | Val Loss: 0.8676\n",
      "Epoch 008 | Train Loss: 0.8861 | Val Loss: 0.8542\n",
      "Epoch 009 | Train Loss: 0.8281 | Val Loss: 0.8546\n",
      "Epoch 010 | Train Loss: 0.8214 | Val Loss: 0.8573\n",
      "Epoch 011 | Train Loss: 0.7886 | Val Loss: 0.8308\n",
      "Epoch 012 | Train Loss: 0.7621 | Val Loss: 0.7096\n",
      "Epoch 013 | Train Loss: 0.8180 | Val Loss: 0.9201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m (F\u001b[38;5;241m.\u001b[39mmse_loss(out, target))\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mGIN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index) \n\u001b[0;32m     23\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h1, edge_index)\n\u001b[1;32m---> 24\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Graph level readout\u001b[39;00m\n\u001b[0;32m     27\u001b[0m h1 \u001b[38;5;241m=\u001b[39m global_add_pool(h1, batch) \n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gin_conv.py:90\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m*\u001b[39m x_r\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MIchele Myong\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f\"\\nğŸ” Retraining Fold {fold+1}/10 with best hyperparameters\")\n",
    "    train_data = torch.load(os.path.join(base_path, f\"{task}_train_fold{fold}.pt\"))\n",
    "    val_data   = torch.load(os.path.join(base_path, f\"{task}_val_fold{fold}.pt\"))\n",
    "\n",
    "    model = GIN(train_data[0].x.size(1),\n",
    "                 hidden_channels=best_hidden_dim,\n",
    "                 out_channels=1,\n",
    "                 dropout=best_dropout).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_data,   batch_size=32)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch).view(-1)\n",
    "            target = batch.y.view(-1).float()\n",
    "            loss = (F.mse_loss(out, target))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation\n",
    "        preds, targets = evaluate(model, val_loader)\n",
    "        val_loss = mean_squared_error(targets.numpy(), preds.numpy())\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(results_dir, f\"fold{fold+1}_model.pt\"))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 10:\n",
    "                print(\"â¹ï¸ Early stopping\")\n",
    "                break\n",
    "\n",
    "    # --- After training this fold, compute metrics on its validation set ---\n",
    "    preds_np = preds.squeeze().cpu().numpy()\n",
    "    trues_np = targets.cpu().numpy()\n",
    "\n",
    "    if task == \"classification\":\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score,\n",
    "            precision_recall_fscore_support,\n",
    "            roc_auc_score\n",
    "        )\n",
    "        y_true = trues_np.astype(int)\n",
    "        y_probs = F.softmax(preds, dim=1).cpu().numpy()\n",
    "        y_pred  = preds.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "        auc = roc_auc_score(\n",
    "            label_binarize(y_true, classes=np.arange(num_classes)),\n",
    "            y_probs, multi_class=\"ovr\")\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold+1,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"auc_roc\": auc\n",
    "        })\n",
    "    else:  # regression\n",
    "        mae  = mean_absolute_error(trues_np, preds_np)\n",
    "        mse  = mean_squared_error(trues_np, preds_np)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2   = r2_score(trues_np, preds_np)\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold+1,\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2\n",
    "        })\n",
    "\n",
    "# --- Save the cross-validation summary ---\n",
    "cv_df = pd.DataFrame(fold_metrics)\n",
    "cv_path = os.path.join(results_dir, \"crossval_summary.csv\")\n",
    "cv_df.to_csv(cv_path, index=False)\n",
    "print(f\"âœ… Saved crossâ€‘validation summary to {cv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039519a2",
   "metadata": {},
   "source": [
    "## 7: Retrain All Folds with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c82235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f\"\\nğŸ” Retraining Fold {fold+1}/10 with best hyperparameters\")\n",
    "    train_data = torch.load(os.path.join(base_path, f\"{task}_train_fold{fold}.pt\"))\n",
    "    val_data   = torch.load(os.path.join(base_path, f\"{task}_val_fold{fold}.pt\"))\n",
    "\n",
    "    model = GIN(in_channels=train_data[0].x.size(1), hidden_channels=hd, out_channels=1, dropout=dp).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_data,   batch_size=32)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 0\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    for epoch in range(1, 301):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = (F.cross_entropy(out, batch.y.long())\n",
    "                    if task==\"classification\"\n",
    "                    else F.mse_loss(out.squeeze(), batch.y))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Evaluate on validation\n",
    "        preds, targets = evaluate(model, val_loader)\n",
    "        val_loss = (F.cross_entropy(preds, targets.long()).item()\n",
    "                    if task==\"classification\"\n",
    "                    else F.mse_loss(preds.squeeze(), targets).item())\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(results_dir, f\"fold{fold+1}_model.pt\"))\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= 10:\n",
    "                print(\"â¹ï¸ Early stopping\")\n",
    "                break\n",
    "\n",
    "    # --- After training this fold, compute metrics on its validation set ---\n",
    "    preds_np = preds.squeeze().cpu().numpy()\n",
    "    trues_np = targets.cpu().numpy()\n",
    "\n",
    "    if task == \"classification\":\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score,\n",
    "            precision_recall_fscore_support,\n",
    "            roc_auc_score\n",
    "        )\n",
    "        y_true = trues_np.astype(int)\n",
    "        y_probs = F.softmax(preds, dim=1).cpu().numpy()\n",
    "        y_pred  = preds.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "        auc = roc_auc_score(\n",
    "            label_binarize(y_true, classes=np.arange(num_classes)),\n",
    "            y_probs, multi_class=\"ovr\")\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold+1,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"auc_roc\": auc\n",
    "        })\n",
    "    else:  # regression\n",
    "        mae  = mean_absolute_error(trues_np, preds_np)\n",
    "        mse  = mean_squared_error(trues_np, preds_np)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2   = r2_score(trues_np, preds_np)\n",
    "\n",
    "        fold_metrics.append({\n",
    "            \"fold\": fold+1,\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2\n",
    "        })\n",
    "\n",
    "# --- Save the cross-validation summary ---\n",
    "cv_df = pd.DataFrame(fold_metrics)\n",
    "cv_path = os.path.join(results_dir, \"crossval_summary.csv\")\n",
    "cv_df.to_csv(cv_path, index=False)\n",
    "print(f\"âœ… Saved crossâ€‘validation summary to {cv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f5411",
   "metadata": {},
   "source": [
    "## 7b: Visualize Cross-Validation Results\n",
    "## This section plots per-fold metrics from the cross-validation summary to assess stability across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe804d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load summary\n",
    "cv_path = os.path.join(results_dir, \"crossval_summary.csv\")\n",
    "cv_df = pd.read_csv(cv_path)\n",
    "\n",
    "# Choose metrics\n",
    "task_metrics = (\n",
    "    ['accuracy','precision','recall','f1_score','auc_roc']\n",
    "    if task==\"classification\"\n",
    "    else ['mae','mse','rmse','r2']\n",
    ")\n",
    "\n",
    "# Plot bar charts per fold\n",
    "fig, axs = plt.subplots(1, len(task_metrics), figsize=(18, 4))\n",
    "for i, metric in enumerate(task_metrics):\n",
    "    axs[i].bar(cv_df['fold'], cv_df[metric], color='skyblue')\n",
    "    axs[i].set_title(f\"{metric.upper()} per Fold\")\n",
    "    axs[i].set_xlabel(\"Fold\")\n",
    "    axs[i].set_ylabel(metric)\n",
    "    axs[i].set_xticks(cv_df['fold'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display mean Â± std\n",
    "summary_stats = cv_df[task_metrics].agg(['mean','std']).T\n",
    "print(\"ğŸ“Š Mean Â± Std for Cross-Validation Metrics:\")\n",
    "display(summary_stats.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59531d85",
   "metadata": {},
   "source": [
    "## 8a: Ensemble Averaging from Cross-Validation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94858c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "ensemble_preds = []\n",
    "test_data = torch.load(os.path.join(base_path, f\"{task}_test.pt\"))\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Ensure model configuration matches\n",
    "output_dim = num_classes if task == \"classification\" else 1\n",
    "\n",
    "# Collect predictions from each fold model\n",
    "for fold in range(10):\n",
    "    GIN(in_channels=train_data[0].x.size(1), hidden_channels=best_hidden_dim, out_channels=1, dropout=best_dropout).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(results_dir, f\"fold{fold+1}_model.pt\")))\n",
    "    model.eval()\n",
    "    fold_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            fold_outputs.append(out.cpu())\n",
    "    ensemble_preds.append(torch.cat(fold_outputs, dim=0))\n",
    "\n",
    "# Average predictions across folds\n",
    "avg_output = torch.stack(ensemble_preds).mean(dim=0)\n",
    "\n",
    "# Prepare final predictions and true labels\n",
    "if task == \"classification\":\n",
    "    final_pred = avg_output.argmax(dim=1).numpy()\n",
    "    true_labels = torch.cat([data.y for data in test_data]).numpy().astype(int)\n",
    "else:\n",
    "    final_pred = avg_output.squeeze().numpy()\n",
    "    true_value = torch.cat([data.y for data in test_data]).numpy()\n",
    "\n",
    "print(f\"âœ… Ensemble predictions ready: shape {final_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86701f",
   "metadata": {},
   "source": [
    "## 8b: Ensemble Evaluation & Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "mae = mean_absolute_error(true_value, final_pred)\n",
    "mse = mean_squared_error  (true_value, final_pred)\n",
    "rmse= np.sqrt(mse)\n",
    "r2  = r2_score           (true_value, final_pred)\n",
    "print(f\"Ensemble: MAE={mae:.3f}, RMSE={rmse:.3f}, R2={r2:.3f}\")\n",
    "plt.figure(figsize=(6,5)); plt.scatter(true_value,final_pred,alpha=0.7)\n",
    "plt.plot([true_value.min(),true_value.max()],[true_value.min(),true_value.max()], 'r--'); plt.xlabel(\"True\"); plt.ylabel(\"Pred\"); plt.title(\"Ensemble True vs Pred\"); plt.grid(True); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d274751f",
   "metadata": {},
   "source": [
    "## 9: Final Model Training on Combined Train+Val & Test Evaluation (Holdâ€‘Out Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e81ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Merge all train+val folds\n",
    "all_data = []\n",
    "for fold in range(10):\n",
    "    all_data += torch.load(os.path.join(base_path, f\"{task}_train_fold{fold}.pt\"))\n",
    "    all_data += torch.load(os.path.join(base_path, f\"{task}_val_fold{fold}.pt\"))\n",
    "\n",
    "# 2) Reserve a small val split for early stopping\n",
    "tidx, vidx = train_test_split(\n",
    "    list(range(len(all_data))),\n",
    "    test_size=0.10,\n",
    "    random_state=seed,\n",
    "    shuffle=True\n",
    ")\n",
    "train_split = [all_data[i] for i in tidx]\n",
    "val_split   = [all_data[i] for i in vidx]\n",
    "\n",
    "train_loader = DataLoader(train_split, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_split,   batch_size=32)\n",
    "\n",
    "# 3) Instantiate & train final model\n",
    "model     = MPNN(input_dim, edge_dim,\n",
    "                 hidden_dim=best_hidden_dim,\n",
    "                 output_dim=output_dim,\n",
    "                 dropout=best_dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', patience=5, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience      = 0\n",
    "\n",
    "for epoch in range(1, 300):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = (F.cross_entropy(out, batch.y.long())\n",
    "                if task == \"classification\"\n",
    "                else F.mse_loss(out.squeeze(), batch.y))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation for early stopping\n",
    "    preds, targets = evaluate(model, val_loader)\n",
    "    val_loss = (F.cross_entropy(preds, targets.long()).item()\n",
    "                if task == \"classification\"\n",
    "                else F.mse_loss(preds.squeeze(), targets).item())\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch:03d} | Train Loss: {total_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience      = 0\n",
    "        torch.save(model.state_dict(), os.path.join(results_dir, \"final_model.pt\"))\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= 10:\n",
    "            print(\"â¹ï¸ Early stopping\")\n",
    "            break\n",
    "\n",
    "# 4) Load best final model & evaluate on ORIGINAL TEST set\n",
    "model.load_state_dict(torch.load(os.path.join(results_dir, \"final_model.pt\")))\n",
    "test_data   = torch.load(os.path.join(base_path, f\"{task}_test.pt\"))\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "preds, targets = evaluate(model, test_loader)\n",
    "pred_final     = (preds.argmax(dim=1).numpy() \n",
    "                  if task==\"classification\" \n",
    "                  else preds.squeeze().numpy())\n",
    "true_final     = torch.cat([d.y for d in test_data]).numpy().astype(int if task==\"classification\" else float)\n",
    "\n",
    "# 5) Compute final metrics\n",
    "if task == \"classification\":\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score,\n",
    "        precision_recall_fscore_support,\n",
    "        roc_auc_score\n",
    "    )\n",
    "    acc       = accuracy_score(true_final, pred_final)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_final, pred_final, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    probs     = F.softmax(preds, dim=1).cpu().numpy()\n",
    "    auc       = roc_auc_score(\n",
    "        label_binarize(true_final, classes=np.arange(num_classes)),\n",
    "        probs, multi_class=\"ovr\"\n",
    "    )\n",
    "    final_metrics = {\n",
    "        \"accuracy\": acc, \"precision\": precision,\n",
    "        \"recall\": recall, \"f1_score\": f1, \"auc_roc\": auc\n",
    "    }\n",
    "else:\n",
    "    mae_f  = mean_absolute_error(true_final, pred_final)\n",
    "    mse_f  = mean_squared_error(true_final, pred_final)\n",
    "    rmse_f = np.sqrt(mse_f)\n",
    "    r2_f   = r2_score(true_final, pred_final)\n",
    "    final_metrics = {\"mae\": mae_f, \"mse\": mse_f, \"rmse\": rmse_f, \"r2\": r2_f}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final metrics to a CSV file\n",
    "final_metrics_df = pd.DataFrame([final_metrics])\n",
    "final_metrics_df.to_csv(os.path.join(results_dir, \"final_metrics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc826d",
   "metadata": {},
   "source": [
    "## 10. Generate plots and compare to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2dcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ensemble metrics list\n",
    "if task == \"classification\":\n",
    "    ensemble_metrics = [acc, precision, recall, f1, auc]\n",
    "else:\n",
    "    ensemble_metrics = [mae, mse, rmse, r2]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comp = pd.DataFrame({\n",
    "    \"metric\":   list(final_metrics.keys()),\n",
    "    \"ensemble\": ensemble_metrics,\n",
    "    \"final\":    list(final_metrics.values())\n",
    "})\n",
    "display(comp)\n",
    "\n",
    "\n",
    "# 8) Save final model\n",
    "model_path = os.path.join(results_dir, \"final_model.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"âœ… Final model saved to {model_path}\")\n",
    "#\n",
    "ensemble_plot_path = os.path.join(results_dir, \"ensemble_plot.png\")\n",
    "fig_ensemble, ax_ensemble = plt.subplots(figsize=(6, 5))\n",
    "ax_ensemble.scatter(true_value, final_pred, alpha=0.7)\n",
    "ax_ensemble.plot(\n",
    "    [true_value.min(), true_value.max()],\n",
    "    [true_value.min(), true_value.max()],\n",
    "    'r--'\n",
    ")\n",
    "ax_ensemble.set_title(\"Ensemble: True vs Predicted\")\n",
    "ax_ensemble.set_xlabel(\"True pChEMBL\")\n",
    "ax_ensemble.set_ylabel(\"Ensemble Prediction\")\n",
    "plt.tight_layout()\n",
    "fig_ensemble.savefig(ensemble_plot_path)\n",
    "print(f\"âœ… Ensemble plot saved to {ensemble_plot_path}\")\n",
    "\n",
    "# Save final model plot\n",
    "final_model_plot_path = os.path.join(results_dir, \"final_model_plot.png\")\n",
    "fig_final, ax_final = plt.subplots(figsize=(6, 5))\n",
    "ax_final.scatter(true_final, pred_final, alpha=0.7)\n",
    "ax_final.plot(\n",
    "    [true_final.min(), true_final.max()],\n",
    "    [true_final.min(), true_final.max()],\n",
    "    'r--'\n",
    ")\n",
    "ax_final.set_title(\"Final Model: True vs Predicted\")\n",
    "ax_final.set_xlabel(\"True pChEMBL\")\n",
    "ax_final.set_ylabel(\"Final Model Prediction\")\n",
    "plt.tight_layout()\n",
    "fig_final.savefig(final_model_plot_path)\n",
    "print(f\"âœ… Final model plot saved to {final_model_plot_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
